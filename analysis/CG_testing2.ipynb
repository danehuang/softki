{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import random\n",
    "from os.path import exists\n",
    "from typing import *\n",
    "\n",
    "# Common data science imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import random_split, DataLoader, Dataset\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Visualization tools\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from seaborn import heatmap\n",
    "from tqdm import tqdm, notebook as tqdm_notebook\n",
    "\n",
    "# GPyTorch and linear_operator imports\n",
    "import gpytorch\n",
    "from gpytorch.kernels import ScaleKernel, RBFKernel\n",
    "from gpytorch.constraints import *\n",
    "import linear_operator\n",
    "from linear_operator.settings import max_cholesky_size\n",
    "from linear_operator.operators.dense_linear_operator import DenseLinearOperator\n",
    "from linear_operator.utils.cholesky import psd_safe_cholesky\n",
    "\n",
    "# Custom soft GP and MLL imports\n",
    "from gp.soft_gp.soft_gp import SoftGP\n",
    "from gp.soft_gp.mll import HutchinsonPseudoLoss\n",
    "from linear_solver.cg import linear_cg\n",
    "\n",
    "# Data analysis and UCI dataset\n",
    "\n",
    "# Utility functions for dataset handling\n",
    "from gp.util import flatten_dataset, split_dataset, filter_param\n",
    "\n",
    "# Experiment tracking\n",
    "import wandb\n",
    "\n",
    "# System path adjustments\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_gp(model, test_dataset: Dataset, device=\"cuda:0\") -> float:\n",
    "    preds = []\n",
    "    neg_mlls = []\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False,num_workers=1)\n",
    "    for x_batch, y_batch in test_loader:\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        preds += [(model.pred(x_batch) - y_batch).detach().cpu()**2]\n",
    "        neg_mlls += [-model.mll(x_batch, y_batch).detach().cpu()]\n",
    "    rmse = torch.sqrt(torch.sum(torch.cat(preds)) / len(test_dataset)).item()\n",
    "    neg_mll = torch.sum(torch.tensor(neg_mlls))\n",
    "            \n",
    "    print(\"RMSE:\", rmse, \"NEG_MLL\", neg_mll.item(), \"NOISE\", model.noise.cpu().item(), \"LENGTHSCALE\", model.get_lengthscale(), \"OUTPUTSCALE\", model.get_outputscale())# \"T\",model.T)\n",
    "    \n",
    "    return {\n",
    "        \"rmse\": rmse,\n",
    "        \"nll\": neg_mll,\n",
    "    }   \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIZE (48827, 21)\n"
     ]
    }
   ],
   "source": [
    "#==================Dataset============================\n",
    "from data.get_uci import ElevatorsDataset,PoleteleDataset,ProteinDataset, KeggDirectedDataset\n",
    "# # dataset = ElevatorsDataset(\"../data/uci_datasets/uci_datasets/elevators/data.csv\")\n",
    "# dataset = PoleteleDataset(\"../data/uci_datasets/uci_datasets/pol/data.csv\")\n",
    "# dataset = ProteinDataset(\"../data/uci_datasets/uci_datasets/protein/data.csv\")\n",
    "dataset = KeggDirectedDataset(\"../data/uci_datasets/uci_datasets/keggdirected/data.csv\")\n",
    "# dataset = CTSlicesDataset(\"../data/uci_datasets/uci_datasets/slice/data.csv\")\n",
    "train_dataset, val_dataset, test_dataset = split_dataset(\n",
    "    dataset,\n",
    "    train_frac=9/10, #TODO change to real vals \n",
    "    val_frac=0/10\n",
    ")\n",
    "\n",
    "def plot_results(all_mean_rmse, all_mean_runtimes, all_std_rmse, all_std_runtimes, epochs,  legend_names):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "    epochs_range = range(1, epochs + 1)\n",
    "\n",
    "    # Plot RMSE per Epoch for each model (baseline + CG tolerances)\n",
    "    for i in range(len(legend_names)):\n",
    "        label = legend_names[i]  # Use the provided names from legend_names list\n",
    "        axes[0].plot(epochs_range, all_mean_rmse[i], label=label)\n",
    "        \n",
    "        # Fill between the RMSE values for standard deviation\n",
    "        axes[0].fill_between(epochs_range,\n",
    "                             [m - s for m, s in zip(all_mean_rmse[i], all_std_rmse[i])],\n",
    "                             [m + s for m, s in zip(all_mean_rmse[i], all_std_rmse[i])],\n",
    "                             alpha=0.3)\n",
    "\n",
    "    axes[0].set_title('RMSE per Epoch')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('RMSE')\n",
    "    axes[0].legend()\n",
    "\n",
    "    # Plot Training Time per Epoch for each model (baseline + CG tolerances)\n",
    "    for i in range(len(legend_names)):\n",
    "        label = legend_names[i]  # Use the provided names from legend_names list\n",
    "        axes[1].plot(epochs_range, all_mean_runtimes[i], label=label)\n",
    "        \n",
    "        # Fill between the runtime values for standard deviation\n",
    "        axes[1].fill_between(epochs_range,\n",
    "                             [m - s for m, s in zip(all_mean_runtimes[i], all_std_runtimes[i])],\n",
    "                             [m + s for m, s in zip(all_mean_runtimes[i], all_std_runtimes[i])],\n",
    "                             alpha=0.3)\n",
    "\n",
    "    axes[1].set_title('Training Time per Epoch')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Time (s)')\n",
    "    # axes[1].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"keggdirected_solvers.png\")\n",
    "    plt.show()\n",
    "\n",
    "def train_gp(GP_class, inducing_points, test_dataset, train_features, train_labels, epochs, device, dtype, model_config=None):\n",
    "    print(device)\n",
    "    print(inducing_points.device)\n",
    "    model_config = model_config or {}\n",
    "    # kernel = RBFKernel().to(device=device, dtype=dtype)\n",
    "    kernel = RBFKernel()\n",
    "    learn_noise = model_config.get(\"learn_noise\", False)\n",
    "    lr = model_config.get(\"learning_rate\", 0.01)\n",
    "    batch_size = model_config.get(\"batch_size\", 1024)\n",
    "\n",
    "    model = GP_class(\n",
    "        kernel,\n",
    "        inducing_points,\n",
    "        noise=model_config.get(\"noise\", 1e-3),\n",
    "        learn_noise=learn_noise,\n",
    "        use_scale=model_config.get(\"use_scale\", True),\n",
    "        dtype=dtype,\n",
    "        device=device,\n",
    "        max_cg_iter=1000,\n",
    "        solver=model_config.get(\"solver\", \"solve\"),\n",
    "        mll_approx=model_config.get(\"mll_approx\", \"hutchinson\"),\n",
    "        fit_chunk_size=model_config.get(\"fit_chunk_size\", 1024),\n",
    "        use_qr=model_config.get(\"use_qr\", True),\n",
    "        hutch_solver = model_config.get(\"hutch_solver\", \"solve\"),\n",
    "    )\n",
    "\n",
    "\n",
    "    epoch_runtimes = []\n",
    "    epoch_rmse = []\n",
    "\n",
    "    # pbar = tqdm(range(epochs), desc=\"Optimizing MLL\")\n",
    "    if learn_noise:\n",
    "        params = model.parameters()\n",
    "    else:\n",
    "        params = filter_param(model.named_parameters(), \"likelihood.noise_covar.raw_noise\")\n",
    "    optimizer = torch.optim.Adam(params, lr=lr)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    def train_model():\n",
    "        #==================Train============================\n",
    "        for _ in tqdm(range(epochs)):\n",
    "            print(\"training current epoch\")\n",
    "            epoch_start_time = time.time()\n",
    "\n",
    "            for x_batch, y_batch in train_loader:\n",
    "                x_batch = x_batch.clone().detach().to(dtype=dtype, device=device)\n",
    "                y_batch = y_batch.clone().detach().to(dtype=dtype, device=device)\n",
    "                optimizer.zero_grad()\n",
    "                with gpytorch.settings.max_root_decomposition_size(100), max_cholesky_size(int(1.e7)), gpytorch.settings.max_preconditioner_size(15):\n",
    "                    neg_mll = -model.mll(x_batch, y_batch)\n",
    "                neg_mll.backward()\n",
    "                optimizer.step()\n",
    "            model.fit(train_features, train_labels)\n",
    "            epoch_end_time = time.time()\n",
    "            epoch_runtimes.append(epoch_end_time - epoch_start_time)\n",
    "\n",
    "            #==================Evaluate============================\n",
    "            print(\"Running eval\")\n",
    "            eval_results = eval_gp(model, test_dataset, device=device)\n",
    "            epoch_rmse.append(eval_results['rmse'])\n",
    "            print(\"eval finished\")    \n",
    "    train_model()\n",
    "    return epoch_rmse, epoch_runtimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark(train_dataset, test_dataset, epochs=2, seed=42, N=3, configs=None):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "    num_inducing = 512\n",
    "    dtype = torch.float32\n",
    "    device = \"cuda:0\"\n",
    "    \n",
    "    all_mean_rmse = []\n",
    "    all_mean_runtimes = []\n",
    "    all_std_rmse = []\n",
    "    all_std_runtimes = []\n",
    "\n",
    "    #==================Inducing Points============================\n",
    "    train_features, train_labels = flatten_dataset(train_dataset)\n",
    "    kmeans = KMeans(n_clusters=num_inducing)\n",
    "    kmeans.fit(train_features)\n",
    "    centers = kmeans.cluster_centers_\n",
    "    inducing_points = torch.tensor(centers).to(dtype=dtype, device=device)\n",
    "\n",
    "    if configs is None:\n",
    "        raise ValueError(\"You must provide a list of configurations in 'configs'\")\n",
    "\n",
    "    for config in configs:\n",
    "        all_runs_rmse = []\n",
    "        all_runs_runtimes = []\n",
    "\n",
    "        solver_name = config.get(\"solver\", \"Unknown Solver\")\n",
    "        print(f\"Running model with solver '{solver_name}'\")\n",
    "\n",
    "        for run in range(N):\n",
    "            epoch_rmse, epoch_runtimes = train_gp(\n",
    "                SoftGP,\n",
    "                inducing_points.clone(),\n",
    "                test_dataset,\n",
    "                train_features,\n",
    "                train_labels,\n",
    "                epochs,\n",
    "                device,\n",
    "                dtype,\n",
    "                model_config=config  # Pass current config\n",
    "            )\n",
    "            all_runs_rmse.append(epoch_rmse)\n",
    "            all_runs_runtimes.append(epoch_runtimes)\n",
    "\n",
    "        # Calculate mean and std deviation across the N runs for the current configuration\n",
    "        mean_rmse = np.mean(all_runs_rmse, axis=0)\n",
    "        std_rmse = np.std(all_runs_rmse, axis=0)\n",
    "        mean_runtimes = np.mean(all_runs_runtimes, axis=0)\n",
    "        std_runtimes = np.std(all_runs_runtimes, axis=0)\n",
    "\n",
    "        all_mean_rmse.append(mean_rmse)\n",
    "        all_mean_runtimes.append(mean_runtimes)\n",
    "        all_std_rmse.append(std_rmse)\n",
    "        all_std_runtimes.append(std_runtimes)\n",
    "\n",
    "    return all_mean_rmse, all_mean_runtimes, all_std_rmse, all_std_runtimes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CG Fit Testing \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58bf3ff5a95246cab057582a625d6ef1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model with solver 'cg'\n",
      "cuda:0\n",
      "cuda:0\n",
      "Using softmax_interp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training current epoch\n",
      "USING PRECONDITIONER\n",
      "Running eval\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/50 [00:05<04:15,  5.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.14271055161952972 NEG_MLL -931.5643310546875 NOISE 0.0010000000474974513 LENGTHSCALE tensor([[0.9304]], grad_fn=<ToCopyBackward0>) OUTPUTSCALE tensor(0.8944, grad_fn=<ToCopyBackward0>)\n",
      "eval finished\n",
      "training current epoch\n",
      "USING PRECONDITIONER\n",
      "Running eval\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/50 [00:09<07:26,  9.12s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m\n\u001b[1;32m     13\u001b[0m N \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;66;03m# Number of runs\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m all_mean_rmse, all_mean_runtimes, all_std_rmse, all_std_runtimes \u001b[38;5;241m=\u001b[39m \u001b[43mbenchmark\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m6535\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfigs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfigs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 33\u001b[0m, in \u001b[0;36mbenchmark\u001b[0;34m(train_dataset, test_dataset, epochs, seed, N, configs)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning model with solver \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msolver_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m run \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(N):\n\u001b[0;32m---> 33\u001b[0m     epoch_rmse, epoch_runtimes \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_gp\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m        \u001b[49m\u001b[43mSoftGP\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m        \u001b[49m\u001b[43minducing_points\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Pass current config\u001b[39;49;00m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m     all_runs_rmse\u001b[38;5;241m.\u001b[39mappend(epoch_rmse)\n\u001b[1;32m     45\u001b[0m     all_runs_runtimes\u001b[38;5;241m.\u001b[39mappend(epoch_runtimes)\n",
      "Cell \u001b[0;32mIn[3], line 115\u001b[0m, in \u001b[0;36mtrain_gp\u001b[0;34m(GP_class, inducing_points, test_dataset, train_features, train_labels, epochs, device, dtype, model_config)\u001b[0m\n\u001b[1;32m    113\u001b[0m         epoch_rmse\u001b[38;5;241m.\u001b[39mappend(eval_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrmse\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval finished\u001b[39m\u001b[38;5;124m\"\u001b[39m)    \n\u001b[0;32m--> 115\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m epoch_rmse, epoch_runtimes\n",
      "Cell \u001b[0;32mIn[3], line 112\u001b[0m, in \u001b[0;36mtrain_gp.<locals>.train_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;66;03m#==================Evaluate============================\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning eval\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 112\u001b[0m eval_results \u001b[38;5;241m=\u001b[39m \u001b[43meval_gp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m epoch_rmse\u001b[38;5;241m.\u001b[39mappend(eval_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrmse\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval finished\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 8\u001b[0m, in \u001b[0;36meval_gp\u001b[0;34m(model, test_dataset, device)\u001b[0m\n\u001b[1;32m      6\u001b[0m     x_batch \u001b[38;5;241m=\u001b[39m x_batch\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      7\u001b[0m     y_batch \u001b[38;5;241m=\u001b[39m y_batch\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 8\u001b[0m     preds \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpred\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my_batch\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m      9\u001b[0m     neg_mlls \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m-\u001b[39mmodel\u001b[38;5;241m.\u001b[39mmll(x_batch, y_batch)\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()]\n\u001b[1;32m     10\u001b[0m rmse \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msqrt(torch\u001b[38;5;241m.\u001b[39msum(torch\u001b[38;5;241m.\u001b[39mcat(preds)) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(test_dataset))\u001b[38;5;241m.\u001b[39mitem()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "configs = [\n",
    "    {\"solver\": \"cg\", \"hutch_solver\": \"solve\",\"cg_tolerance\": 1e-4,\"use_qr\": False},\n",
    "    {\"solver\": \"cg\", \"hutch_solver\": \"solve\",\"cg_tolerance\": 1e-3,\"use_qr\": False},\n",
    "    {\"solver\": \"cg\", \"hutch_solver\": \"solve\",\"cg_tolerance\": 1e-2,\"use_qr\": False},\n",
    "    {\"solver\": \"cg\", \"hutch_solver\": \"solve\",\"cg_tolerance\": 1e-1,\"use_qr\": False},\n",
    "    {\"solver\": \"solve\", \"hutch_solver\": \"solve\",\"cg_tolerance\": 1e-4,\"use_qr\": True},\n",
    "    {\"solver\": \"cholesky\", \"hutch_solver\": \"solve\",\"use_qr\": False},\n",
    "    {\"solver\": \"solve\", \"hutch_solver\": \"solve\",\"use_qr\": False},\n",
    "]\n",
    "legend_names = ['QR Solve', 'CG Solver 1e-4','CG Solver 1e-3','CG Solver 1e-2','CG Solver 1e-1','Cholesky Solver',\"Direct Solver\"]\n",
    "\n",
    "epochs = 50\n",
    "N = 1 # Number of runs\n",
    "all_mean_rmse, all_mean_runtimes, all_std_rmse, all_std_runtimes = benchmark(train_dataset, test_dataset, epochs=epochs, seed=6535, N=N, configs=configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results2(all_mean_rmse, all_mean_runtimes, all_std_rmse, all_std_runtimes, epochs,  legend_names):\n",
    "    fig, axes = plt.subplots(1, 1, figsize=(6, 6))\n",
    "\n",
    "    epochs_range = range(1, epochs + 1)\n",
    "\n",
    "    # Plot RMSE per Epoch for each model (baseline + CG tolerances)\n",
    "    for i in range(len(legend_names)):\n",
    "        label = legend_names[i]  # Use the provided names from legend_names list\n",
    "        if \"CG\" in label:\n",
    "            style = \"-\"\n",
    "        elif \"Cholesky\" in label:\n",
    "            style = \"--\"\n",
    "        else:\n",
    "            style = \":\"\n",
    "        if \"QR\" in label:\n",
    "            width = 3\n",
    "        elif \"CG\" in label:\n",
    "            width = 1\n",
    "        else:\n",
    "            width = 1\n",
    "        axes.plot(epochs_range, all_mean_rmse[i], label=label, linestyle=style, linewidth=width)\n",
    "        \n",
    "        # Fill between the RMSE values for standard deviation\n",
    "        axes.fill_between(epochs_range,\n",
    "                             [m - s for m, s in zip(all_mean_rmse[i], all_std_rmse[i])],\n",
    "                             [m + s for m, s in zip(all_mean_rmse[i], all_std_rmse[i])],\n",
    "                             alpha=0.3)\n",
    "\n",
    "    axes.set_title('RMSE per Epoch')\n",
    "    axes.set_xlabel('Epoch', fontsize=12)\n",
    "    axes.set_ylabel('Test RMSE', fontsize=12)\n",
    "    axes.tick_params(axis='both', which='major', labelsize=12)\n",
    "\n",
    "    # axes.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('keggdirected_solvers.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "legend_names = ['CG Solver 1e-4','CG Solver 1e-3','CG Solver 1e-2','CG Solver 1e-1','QR Solve','Cholesky Solver',\"Direct Solver\"]\n",
    "# tolerance_values = [None,1e-4, 1e-3, 1e-2, 1e-1]\n",
    "plot_results2(all_mean_rmse, all_mean_runtimes, all_std_rmse, all_std_runtimes, epochs, legend_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "softgp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
