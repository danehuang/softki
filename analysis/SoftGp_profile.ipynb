{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chris\\anaconda3\\envs\\softgp\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import time \n",
    "from tqdm import tqdm \n",
    "import random \n",
    "import torch \n",
    "from torch.utils.data import random_split, DataLoader, Dataset\n",
    "import gpytorch\n",
    "from gpytorch.kernels import ScaleKernel, RBFKernel\n",
    "from sklearn.cluster import KMeans\n",
    "from linear_operator.settings import max_cholesky_size\n",
    "\n",
    "import sys \n",
    "sys.path.append(\"../\")\n",
    "from gp.util import dynamic_instantiation, flatten_dict, unflatten_dict, flatten_dataset, split_dataset, filter_param, heatmap\n",
    "\n",
    "# System/Library imports\n",
    "from typing import *\n",
    "\n",
    "# Common data science imports\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Gpytorch and linear_operator\n",
    "import gpytorch \n",
    "import gpytorch.constraints\n",
    "from gpytorch.kernels import ScaleKernel\n",
    "import linear_operator\n",
    "from linear_operator.operators.dense_linear_operator import DenseLinearOperator\n",
    "from linear_operator.utils.cholesky import psd_safe_cholesky\n",
    "\n",
    "# Our imports\n",
    "from gp.soft_gp.mll import HutchinsonPseudoLoss\n",
    "from linear_solver.cg import linear_cg\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soft GP testing profiling/tuning boilerplate\n",
    "its recommend to collapse all functions with Ctrl/Cmnd +k +0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SoftGP baseline implementation \n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftGP_baseline(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        kernel: Callable,\n",
    "        inducing_points: torch.Tensor,\n",
    "        noise=1e-3,\n",
    "        learn_noise=False,\n",
    "        use_scale=False,\n",
    "        device=\"cpu\",\n",
    "        dtype=torch.float32,\n",
    "        solver=\"solve\",\n",
    "        max_cg_iter=50,\n",
    "        cg_tolerance=0.5,\n",
    "        mll_approx=\"hutchinson\",\n",
    "        fit_chunk_size=1024,\n",
    "        use_qr=False,\n",
    "    ) -> None:\n",
    "        # Argument checking \n",
    "        methods = [\"solve\", \"cholesky\", \"cg\"]\n",
    "        if not solver in methods:\n",
    "            raise ValueError(f\"Method {solver} should be in {methods} ...\")\n",
    "        \n",
    "        # Check devices\n",
    "        devices = [\"cpu\"]\n",
    "        if torch.cuda.is_available():\n",
    "            devices += [\"cuda\"]\n",
    "            for i in range(torch.cuda.device_count()):\n",
    "                devices += [f\"cuda:{i}\"]\n",
    "        if not device in devices:\n",
    "            raise ValueError(f\"Device {device} should be in {devices} ...\")\n",
    "\n",
    "        # Create torch module\n",
    "        super(SoftGP_baseline, self).__init__()\n",
    "\n",
    "        # Misc\n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "        \n",
    "        # Mll approximation settings\n",
    "        self.solve_method = solver\n",
    "        self.mll_approx = mll_approx\n",
    "\n",
    "        # Fit settings\n",
    "        self.use_qr = use_qr\n",
    "        self.fit_chunk_size = fit_chunk_size\n",
    "\n",
    "        # Noise\n",
    "        self.noise_constraint = gpytorch.constraints.Positive()\n",
    "        noise = torch.tensor([noise], dtype=self.dtype, device=self.device)\n",
    "        noise = self.noise_constraint.inverse_transform(noise)\n",
    "        if learn_noise:\n",
    "            self.register_parameter(\"raw_noise\", torch.nn.Parameter(noise))\n",
    "        else:\n",
    "            self.raw_noise = noise\n",
    "\n",
    "        # Kernel\n",
    "        self.use_scale = use_scale\n",
    "        if use_scale:\n",
    "            self.kernel = ScaleKernel(kernel).to(self.device)\n",
    "        else:\n",
    "            self.kernel = kernel.to(self.device)\n",
    "\n",
    "        # Inducing points\n",
    "        self.register_parameter(\"inducing_points\", torch.nn.Parameter(inducing_points))\n",
    "\n",
    "        # Interpolation\n",
    "        def softmax_interp(X: torch.Tensor, sigma_values: torch.Tensor) -> torch.Tensor:\n",
    "            distances = torch.linalg.vector_norm(X - sigma_values, ord=2, dim=-1)\n",
    "            softmax_distances = torch.softmax(-distances, dim=-1)\n",
    "            return softmax_distances\n",
    "        self.interp = softmax_interp\n",
    "        \n",
    "        # Fit artifacts\n",
    "        self.alpha = None\n",
    "        self.K_zz_alpha = None\n",
    "\n",
    "        # CG solver params\n",
    "        self.max_cg_iter = max_cg_iter\n",
    "        self.cg_tol = cg_tolerance\n",
    "        self.x0 = None\n",
    "        \n",
    "    # -----------------------------------------------------\n",
    "    # Soft GP Helpers\n",
    "    # -----------------------------------------------------\n",
    "    \n",
    "    @property\n",
    "    def noise(self):\n",
    "        return self.noise_constraint.transform(self.raw_noise)\n",
    "\n",
    "    def get_lengthscale(self) -> float:\n",
    "        if self.use_scale:\n",
    "            return self.kernel.base_kernel.lengthscale.cpu()\n",
    "        else:\n",
    "            return self.kernel.lengthscale.cpu()\n",
    "        \n",
    "    def get_outputscale(self) -> float:\n",
    "        if self.use_scale:\n",
    "            return self.kernel.outputscale.cpu()\n",
    "        else:\n",
    "            return 1.\n",
    "\n",
    "    def _mk_cov(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        return self.kernel(z, z).evaluate()\n",
    "    \n",
    "    def _interp(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x_expanded = x.unsqueeze(1).expand(-1, self.inducing_points.shape[0], -1)\n",
    "        W_xz = self.interp(x_expanded, self.inducing_points)\n",
    "        return W_xz\n",
    "\n",
    "    # -----------------------------------------------------\n",
    "    # Linear solver\n",
    "    # -----------------------------------------------------\n",
    "\n",
    "    def _solve_system(\n",
    "        self,\n",
    "        kxx: linear_operator.operators.LinearOperator,\n",
    "        full_rhs: torch.Tensor,\n",
    "        x0: torch.Tensor = None,\n",
    "        forwards_matmul: Callable = None,\n",
    "        precond: torch.Tensor = None,\n",
    "        return_pinv: bool = False,\n",
    "    ) -> torch.Tensor:\n",
    "        use_pinv = False\n",
    "        with torch.no_grad():\n",
    "            try:\n",
    "                if self.solve_method == \"solve\":\n",
    "                    solve = torch.linalg.solve(kxx, full_rhs)\n",
    "                elif self.solve_method == \"cholesky\":\n",
    "                    L = torch.linalg.cholesky(kxx)\n",
    "                    solve = torch.cholesky_solve(full_rhs, L)\n",
    "                elif self.solve_method == \"cg\":\n",
    "                    # Source: https://github.com/AndPotap/halfpres_gps/blob/main/mlls/mixedpresmll.py\n",
    "                    solve = linear_cg(\n",
    "                        forwards_matmul,\n",
    "                        full_rhs,\n",
    "                        max_iter=self.max_cg_iter,\n",
    "                        tolerance=self.cg_tol,\n",
    "                        initial_guess=x0,\n",
    "                        preconditioner=precond,\n",
    "                    )\n",
    "                else:\n",
    "                    raise ValueError(f\"Unknown method: {self.solve_method}\")\n",
    "            except RuntimeError as e:\n",
    "                print(\"Fallback to pseudoinverse: \", str(e))\n",
    "                solve = torch.linalg.pinv(kxx.evaluate()) @ full_rhs\n",
    "                use_pinv = True\n",
    "\n",
    "        # Apply torch.nan_to_num to handle NaNs from percision limits \n",
    "        solve = torch.nan_to_num(solve)\n",
    "        return (solve, use_pinv) if return_pinv else solve\n",
    "\n",
    "    # -----------------------------------------------------\n",
    "    # Marginal Log Likelihood\n",
    "    # -----------------------------------------------------\n",
    "\n",
    "    def mll(self, X: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Compute the marginal log likelihood of a soft GP:\n",
    "            \n",
    "            log p(y) = log N(y | mu_x, Q_xx)\n",
    "\n",
    "            where\n",
    "                mu_X: mean of soft GP\n",
    "                Q_XX = W_xz K_zz W_zx\n",
    "\n",
    "        Args:\n",
    "            X (torch.Tensor): B x D tensor of inputs where each row is a point.\n",
    "            y (torch.Tensor): B tensor of targets.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor:  log p(y)\n",
    "        \"\"\"        \n",
    "        # Construct covariance matrix components\n",
    "        K_zz = self._mk_cov(self.inducing_points)\n",
    "        W_xz = self._interp(X)\n",
    "        \n",
    "        if self.mll_approx == \"exact\":\n",
    "            # [Note]: Compute MLL with a multivariate normal. Unstable for float.\n",
    "            # 1. mean: 0\n",
    "            mean = torch.zeros(len(X), dtype=self.dtype, device=self.device)\n",
    "            \n",
    "            # 2. covariance: Q_xx = (W_xz L) (L^T W_xz) + noise I  where K_zz = L L^T\n",
    "            L = psd_safe_cholesky(K_zz)\n",
    "            LK = (W_xz @ L).to(device=self.device)\n",
    "            cov_diag = self.noise * torch.ones(len(X), dtype=self.dtype, device=self.device)\n",
    "\n",
    "            # 3. N(mu, Q_xx)\n",
    "            normal_dist = torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal(mean, LK, cov_diag, validate_args=None)\n",
    "            \n",
    "            # 4. log N(y | mu, Q_xx)\n",
    "            return normal_dist.log_prob(y)\n",
    "        elif self.mll_approx == \"hutchinson\":\n",
    "            # [Note]: Compute MLL with Hutchinson's trace estimator\n",
    "            # 1. mean: 0\n",
    "            mean = torch.zeros(len(X), dtype=self.dtype, device=self.device)\n",
    "            \n",
    "            # 2. covariance: Q_xx = W_xz K_zz K_zx + noise I\n",
    "            cov_mat = W_xz @ K_zz @ W_xz.T \n",
    "            cov_mat += torch.eye(cov_mat.shape[1], dtype=self.dtype, device=self.device) * self.noise\n",
    "\n",
    "            # 3. log N(y | mu, Q_xx) \\appox \n",
    "            hutchinson_mll = HutchinsonPseudoLoss(self, num_trace_samples=10)\n",
    "            return hutchinson_mll(mean, cov_mat, y)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown MLL approximation method: {self.mll_approx}\")\n",
    "        \n",
    "    # -----------------------------------------------------\n",
    "    # Fit\n",
    "    # -----------------------------------------------------\n",
    "\n",
    "    def _direct_solve_fit(self, M, N, X, y, K_zz):\n",
    "        # Construct A and b for linear solve\n",
    "        #   A = (K_zz + hat{K}_zx @ noise^{-1} @ hat{K}_xz)\n",
    "        #   b = (hat{K}_zx @ noise^{-1}) y\n",
    "        if X.shape[0] * X.shape[1] <= 32768:\n",
    "            # Case: \"small\" X\n",
    "            # Form estimate \\hat{K}_xz ~= W_xz K_zz\n",
    "            W_xz = self._interp(X)\n",
    "            hat_K_xz = W_xz @ K_zz\n",
    "            hat_K_zx = hat_K_xz.T\n",
    "            \n",
    "            # Form A and b\n",
    "            Lambda_inv_diag = (1 / self.noise) * torch.ones(N, dtype=self.dtype).to(self.device)\n",
    "            A = K_zz + hat_K_zx @ (Lambda_inv_diag.unsqueeze(1) * hat_K_xz)\n",
    "            b = hat_K_zx @ (Lambda_inv_diag * y)\n",
    "        else:\n",
    "            # Case: \"large\" X\n",
    "            with torch.no_grad():\n",
    "                # Initialize outputs\n",
    "                A = torch.zeros(M, M, dtype=self.dtype, device=self.device)\n",
    "                b = torch.zeros(M, dtype=self.dtype, device=self.device)\n",
    "                \n",
    "                # Initialize temporary values\n",
    "                fit_chunk_size = self.fit_chunk_size\n",
    "                batches = int(np.floor(N / fit_chunk_size))\n",
    "                Lambda_inv = (1 / self.noise) * torch.eye(fit_chunk_size, dtype=self.dtype, device=self.device)\n",
    "                tmp1 = torch.zeros(fit_chunk_size, M, dtype=self.dtype, device=self.device)\n",
    "                tmp2 = torch.zeros(M, M, dtype=self.dtype, device=self.device)\n",
    "                tmp3 = torch.zeros(fit_chunk_size, dtype=self.dtype, device=self.device)\n",
    "                tmp4 = torch.zeros(M, dtype=self.dtype, device=self.device)\n",
    "                tmp5 = torch.zeros(M, dtype=self.dtype, device=self.device)\n",
    "                \n",
    "                # Compute batches\n",
    "                for i in range(batches):\n",
    "                    # Update A: A += W_zx @ Lambda_inv @ W_xz\n",
    "                    X_batch = X[i*fit_chunk_size:(i+1)*fit_chunk_size]\n",
    "                    W_xz = self._interp(X_batch)\n",
    "                    W_zx = W_xz.T\n",
    "                    torch.matmul(Lambda_inv, W_xz, out=tmp1)\n",
    "                    torch.matmul(W_zx, tmp1, out=tmp2)\n",
    "                    A.add_(tmp2)\n",
    "                    \n",
    "                    # Update b: b += K_zz @ W_zx @ (Lambda_inv @ Y[i*batch_size:(i+1)*batch_size])\n",
    "                    torch.matmul(Lambda_inv, y[i*fit_chunk_size:(i+1)*fit_chunk_size], out=tmp3)\n",
    "                    torch.matmul(W_zx, tmp3, out=tmp4)\n",
    "                    torch.matmul(K_zz, tmp4, out=tmp5)\n",
    "                    b.add_(tmp5)\n",
    "                \n",
    "                # Compute last batch\n",
    "                if N - (i+1)*fit_chunk_size > 0:\n",
    "                    Lambda_inv = (1 / self.noise) * torch.eye(N - (i+1)*fit_chunk_size, dtype=self.dtype, device=self.device)\n",
    "                    X_batch = X[(i+1)*fit_chunk_size:]\n",
    "                    W_xz = self._interp(X_batch)\n",
    "                    A += W_xz.T @ Lambda_inv @ W_xz\n",
    "                    b += K_zz @ W_xz.T @ Lambda_inv @ y[(i+1)*fit_chunk_size:]\n",
    "\n",
    "                # Aggregate result\n",
    "                A = K_zz + K_zz @ A @ K_zz\n",
    "\n",
    "        # Safe solve A \\alpha = b\n",
    "        A = DenseLinearOperator(A)\n",
    "        self.alpha, use_pinv = self._solve_system(\n",
    "            A,\n",
    "            b.unsqueeze(1),\n",
    "            x0=torch.zeros_like(b),\n",
    "            forwards_matmul=A.matmul,\n",
    "            precond=None,\n",
    "            return_pinv=True\n",
    "        )\n",
    "\n",
    "        # Store for fast prediction\n",
    "        self.K_zz_alpha = K_zz @ self.alpha\n",
    "        return use_pinv\n",
    "\n",
    "    def _qr_solve_fit(self, M, N, X, y, K_zz):\n",
    "        if X.shape[0] * X.shape[1] <= 32768:\n",
    "            # Compute: W_xz K_zz\n",
    "            print(\"USING QR SMALL\")\n",
    "            W_xz = self._interp(X)\n",
    "            hat_K_xz = W_xz @ K_zz\n",
    "        else:\n",
    "            # Compute: W_xz K_zz in a batched fashion\n",
    "            print(\"USING QR BATCH\")\n",
    "            with torch.no_grad():\n",
    "                # Compute batches\n",
    "                fit_chunk_size = self.fit_chunk_size\n",
    "                batches = int(np.floor(N / fit_chunk_size))\n",
    "                Lambda_half_inv_diag = (1 / torch.sqrt(self.noise)) * torch.ones(fit_chunk_size, dtype=self.dtype, device=self.device)\n",
    "                hat_K_xz = torch.zeros((N, M), dtype=self.dtype, device=self.device)\n",
    "                for i in range(batches):\n",
    "                    start = i*fit_chunk_size\n",
    "                    end = (i+1)*fit_chunk_size\n",
    "                    X_batch = X[start:end,:]\n",
    "                    W_xz = self._interp(X_batch)\n",
    "                    torch.matmul(W_xz, K_zz, out=hat_K_xz[start:end,:])\n",
    "                \n",
    "                start = (i+1)*fit_chunk_size\n",
    "                if N - start > 0:\n",
    "                    Lambda_half_inv_diag = (1 / torch.sqrt(self.noise)) * torch.eye(N - (i+1)*fit_chunk_size, dtype=self.dtype, device=self.device)\n",
    "                    X_batch = X[start:]\n",
    "                    W_xz = self._interp(X_batch)\n",
    "                    torch.matmul(W_xz, K_zz, out=hat_K_xz[start:,:])\n",
    "        \n",
    "        # B^T = [(Lambda^{-1/2} \\hat{K}_xz) U_zz ]\n",
    "        U_zz = psd_safe_cholesky(K_zz, upper=True, max_tries=10)\n",
    "        Lambda_half_inv_diag = (1 / torch.sqrt(self.noise)) * torch.ones(N, dtype=self.dtype).to(self.device)\n",
    "        B = torch.cat([Lambda_half_inv_diag.unsqueeze(1) * hat_K_xz, U_zz], dim=0)\n",
    "\n",
    "        # B = QR\n",
    "        Q, R = torch.linalg.qr(B)\n",
    "\n",
    "        # \\alpha = R^{-1} @ Q^T @ Lambda^{-1/2}b\n",
    "        b = Lambda_half_inv_diag * y\n",
    "        self.alpha = torch.linalg.solve_triangular(R, (Q.T[:, 0:N] @ b).unsqueeze(1), upper=True).squeeze(1) # (should use triangular solve)\n",
    "        # self.alpha = ((torch.linalg.inv(R) @ Q.T)[:, :N] @ b)\n",
    "        \n",
    "        # Store for fast inference\n",
    "        self.K_zz_alpha = K_zz @ self.alpha\n",
    "\n",
    "        return False\n",
    "\n",
    "    def fit(self, X: torch.Tensor, y: torch.Tensor) -> bool:\n",
    "        \"\"\"Fits a SoftGP to dataset (X, y). That is, solve:\n",
    "\n",
    "                (hat{K}_zx @ noise^{-1}) y = (K_zz + hat{K}_zx @ noise^{-1} @ hat{K}_xz) \\alpha\n",
    "        \n",
    "            for \\alpha where\n",
    "            1. inducing points z are fixed,\n",
    "            2. hat{K}_zx = K_zz W_zx, and\n",
    "            3. hat{K}_xz = hat{K}_zx^T.\n",
    "\n",
    "        Args:\n",
    "            X (torch.Tensor): N x D tensor of inputs\n",
    "            y (torch.Tensor): N tensor of outputs\n",
    "\n",
    "        Returns:\n",
    "            bool: Returns true if the pseudoinverse was used, false otherwise.\n",
    "        \"\"\"        \n",
    "        # Prepare inputs\n",
    "        N = len(X)\n",
    "        M = len(self.inducing_points)\n",
    "        X = X.to(self.device, dtype=self.dtype)\n",
    "        y = y.to(self.device, dtype=self.dtype)\n",
    "\n",
    "        # Form K_zz\n",
    "        K_zz = self._mk_cov(self.inducing_points)\n",
    "\n",
    "        if self.use_qr:\n",
    "            return self._qr_solve_fit(M, N, X, y, K_zz)\n",
    "        else:\n",
    "            return self._direct_solve_fit(M, N, X, y, K_zz)\n",
    "\n",
    "    # -----------------------------------------------------\n",
    "    # Predict\n",
    "    # -----------------------------------------------------\n",
    "\n",
    "    def pred(self, x_star: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Give the posterior predictive:\n",
    "        \n",
    "            p(y_star | x_star, X, y) \n",
    "                = W_star_z (K_zz \\alpha)\n",
    "                = W_star_z K_zz (K_zz + hat{K}_zx @ noise^{-1} @ hat{K}_xz)^{-1} (hat{K}_zx @ noise^{-1}) y\n",
    "\n",
    "        Args:\n",
    "            x_star (torch.Tensor): B x D tensor of points to evaluate at.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: B tensor of p(y_star | x_star, X, y).\n",
    "        \"\"\"        \n",
    "        W_star_z = self._interp(x_star)\n",
    "        return torch.matmul(W_star_z, self.K_zz_alpha).squeeze(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SoftGP test implementation \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftGP_test(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        kernel: Callable,\n",
    "        inducing_points: torch.Tensor,\n",
    "        noise=1e-3,\n",
    "        learn_noise=False,\n",
    "        use_scale=False,\n",
    "        device=\"cpu\",\n",
    "        dtype=torch.float32,\n",
    "        solver=\"solve\",\n",
    "        max_cg_iter=50,\n",
    "        cg_tolerance=0.5,\n",
    "        mll_approx=\"hutchinson\",\n",
    "        fit_chunk_size=1024,\n",
    "        use_qr=False,\n",
    "    ) -> None:\n",
    "        # Argument checking \n",
    "        methods = [\"solve\", \"cholesky\", \"cg\"]\n",
    "        if not solver in methods:\n",
    "            raise ValueError(f\"Method {solver} should be in {methods} ...\")\n",
    "        \n",
    "        # Check devices\n",
    "        devices = [\"cpu\"]\n",
    "        if torch.cuda.is_available():\n",
    "            devices += [\"cuda\"]\n",
    "            for i in range(torch.cuda.device_count()):\n",
    "                devices += [f\"cuda:{i}\"]\n",
    "        if not device in devices:\n",
    "            raise ValueError(f\"Device {device} should be in {devices} ...\")\n",
    "\n",
    "        # Create torch module\n",
    "        super(SoftGP_test, self).__init__()\n",
    "\n",
    "        # Misc\n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "        \n",
    "        # Mll approximation settings\n",
    "        self.solve_method = solver\n",
    "        self.mll_approx = mll_approx\n",
    "\n",
    "        # Fit settings\n",
    "        self.use_qr = use_qr\n",
    "        self.fit_chunk_size = fit_chunk_size\n",
    "\n",
    "        # Noise\n",
    "        self.noise_constraint = gpytorch.constraints.Positive()\n",
    "        noise = torch.tensor([noise], dtype=self.dtype, device=self.device)\n",
    "        noise = self.noise_constraint.inverse_transform(noise)\n",
    "        if learn_noise:\n",
    "            self.register_parameter(\"raw_noise\", torch.nn.Parameter(noise))\n",
    "        else:\n",
    "            self.raw_noise = noise\n",
    "\n",
    "        # Kernel\n",
    "        self.use_scale = use_scale\n",
    "        if use_scale:\n",
    "            self.kernel = ScaleKernel(kernel).to(self.device)\n",
    "        else:\n",
    "            self.kernel = kernel.to(self.device)\n",
    "\n",
    "        # Inducing points\n",
    "        self.register_parameter(\"inducing_points\", torch.nn.Parameter(inducing_points))\n",
    "\n",
    "        # Interpolation\n",
    "        #self.T = torch.nn.Parameter(torch.tensor(.005))\n",
    "\n",
    "        def softmax_interp( X: torch.Tensor, sigma_values: torch.Tensor) -> torch.Tensor:\n",
    "            X = X / .001 # Use the learnable T\n",
    "            distances = torch.linalg.vector_norm(X - sigma_values, ord=2, dim=-1)\n",
    "            softmax_distances = torch.softmax(-distances, dim=-1)\n",
    "            softmax_distances = torch.where(softmax_distances < 1e-16, torch.tensor(0.0, dtype=softmax_distances.dtype), softmax_distances)\n",
    "            \n",
    "            return softmax_distances\n",
    "\n",
    "        self.interp = softmax_interp\n",
    "        \n",
    "        # Fit artifacts\n",
    "        self.alpha = None\n",
    "        self.K_zz_alpha = None\n",
    "\n",
    "        # CG solver params\n",
    "        self.max_cg_iter = max_cg_iter\n",
    "        self.cg_tol = cg_tolerance\n",
    "        self.x0 = None\n",
    "        \n",
    "    # -----------------------------------------------------\n",
    "    # Soft GP Helpers\n",
    "    # -----------------------------------------------------\n",
    "    \n",
    "    @property\n",
    "    def noise(self):\n",
    "        return self.noise_constraint.transform(self.raw_noise)\n",
    "\n",
    "    def get_lengthscale(self) -> float:\n",
    "        if self.use_scale:\n",
    "            return self.kernel.base_kernel.lengthscale.cpu()\n",
    "        else:\n",
    "            return self.kernel.lengthscale.cpu()\n",
    "        \n",
    "    def get_outputscale(self) -> float:\n",
    "        if self.use_scale:\n",
    "            return self.kernel.outputscale.cpu()\n",
    "        else:\n",
    "            return 1.\n",
    "\n",
    "    def _mk_cov(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        return self.kernel(z, z).evaluate()\n",
    "    \n",
    "    # def _interp(self, x: torch.Tensor) -> torch.Tensor:\n",
    "    #     # Expand input x and perform interpolation to get the dense matrix W_xz_dense\n",
    "    #     x_expanded = x.unsqueeze(1).expand(-1, self.inducing_points.shape[0], -1)\n",
    "    #     W_xz_dense = self.interp(x_expanded, self.inducing_points)\n",
    "        \n",
    "    #     non_zero_indices = W_xz_dense.nonzero(as_tuple=False).t()  \n",
    "    #     non_zero_values = W_xz_dense[non_zero_indices[0], non_zero_indices[1]]  \n",
    "        \n",
    "    #     W_xz_sparse = torch.sparse_coo_tensor(\n",
    "    #         non_zero_indices, non_zero_values, W_xz_dense.size(), device=W_xz_dense.device\n",
    "    #     )\n",
    "        \n",
    "    #     W_xz_sparse = W_xz_sparse.coalesce()\n",
    "        \n",
    "    #     return W_xz_sparse\n",
    "\n",
    "    def _interp(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x_expanded = x.unsqueeze(1).expand(-1, self.inducing_points.shape[0], -1)\n",
    "        W_xz = self.interp(x_expanded, self.inducing_points)\n",
    "        return W_xz\n",
    "\n",
    "    # -----------------------------------------------------\n",
    "    # Linear solver\n",
    "    # -----------------------------------------------------\n",
    "\n",
    "    def _solve_system(\n",
    "        self,\n",
    "        kxx: linear_operator.operators.LinearOperator,\n",
    "        full_rhs: torch.Tensor,\n",
    "        x0: torch.Tensor = None,\n",
    "        forwards_matmul: Callable = None,\n",
    "        precond: torch.Tensor = None,\n",
    "        return_pinv: bool = False,\n",
    "    ) -> torch.Tensor:\n",
    "        use_pinv = False\n",
    "        with torch.no_grad():\n",
    "            try:\n",
    "                if self.solve_method == \"solve\":\n",
    "                    solve = torch.linalg.solve(kxx, full_rhs)\n",
    "                elif self.solve_method == \"cholesky\":\n",
    "                    L = torch.linalg.cholesky(kxx)\n",
    "                    solve = torch.cholesky_solve(full_rhs, L)\n",
    "                elif self.solve_method == \"cg\":\n",
    "                    # Source: https://github.com/AndPotap/halfpres_gps/blob/main/mlls/mixedpresmll.py\n",
    "                    solve = linear_cg(\n",
    "                        forwards_matmul,\n",
    "                        full_rhs,\n",
    "                        max_iter=self.max_cg_iter,\n",
    "                        tolerance=self.cg_tol,\n",
    "                        initial_guess=x0,\n",
    "                        preconditioner=precond,\n",
    "                    )\n",
    "                else:\n",
    "                    raise ValueError(f\"Unknown method: {self.solve_method}\")\n",
    "            except RuntimeError as e:\n",
    "                print(\"Fallback to pseudoinverse: \", str(e))\n",
    "                solve = torch.linalg.pinv(kxx.evaluate()) @ full_rhs\n",
    "                use_pinv = True\n",
    "\n",
    "        # Apply torch.nan_to_num to handle NaNs from percision limits \n",
    "        solve = torch.nan_to_num(solve)\n",
    "        return (solve, use_pinv) if return_pinv else solve\n",
    "\n",
    "    # -----------------------------------------------------\n",
    "    # Marginal Log Likelihood\n",
    "    # -----------------------------------------------------\n",
    "\n",
    "    def mll(self, X: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Compute the marginal log likelihood of a soft GP:\n",
    "            \n",
    "            log p(y) = log N(y | mu_x, Q_xx)\n",
    "\n",
    "            where\n",
    "                mu_X: mean of soft GP\n",
    "                Q_XX = W_xz K_zz W_zx\n",
    "\n",
    "        Args:\n",
    "            X (torch.Tensor): B x D tensor of inputs where each row is a point.\n",
    "            y (torch.Tensor): B tensor of targets.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor:  log p(y)\n",
    "        \"\"\"        \n",
    "        # Construct covariance matrix components\n",
    "        K_zz = self._mk_cov(self.inducing_points)\n",
    "        W_xz = self._interp(X)\n",
    "        \n",
    "        if self.mll_approx == \"exact\":\n",
    "            # [Note]: Compute MLL with a multivariate normal. Unstable for float.\n",
    "            # 1. mean: 0\n",
    "            mean = torch.zeros(len(X), dtype=self.dtype, device=self.device)\n",
    "            \n",
    "            # 2. covariance: Q_xx = (W_xz L) (L^T W_xz) + noise I  where K_zz = L L^T\n",
    "            L = psd_safe_cholesky(K_zz)\n",
    "            LK = (W_xz @ L).to(device=self.device)\n",
    "            cov_diag = self.noise * torch.ones(len(X), dtype=self.dtype, device=self.device)\n",
    "\n",
    "            # 3. N(mu, Q_xx)\n",
    "            normal_dist = torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal(mean, LK, cov_diag, validate_args=None)\n",
    "            \n",
    "            # 4. log N(y | mu, Q_xx)\n",
    "            return normal_dist.log_prob(y)\n",
    "        elif self.mll_approx == \"hutchinson\":\n",
    "            # [Note]: Compute MLL with Hutchinson's trace estimator\n",
    "            # 1. mean: 0\n",
    "            mean = torch.zeros(len(X), dtype=self.dtype, device=self.device)\n",
    "            \n",
    "            # 2. covariance: Q_xx = W_xz K_zz K_zx + noise I\n",
    "            cov_mat = W_xz @ K_zz @ W_xz.T \n",
    "            cov_mat += torch.eye(cov_mat.shape[1], dtype=self.dtype, device=self.device) * self.noise\n",
    "\n",
    "            # 3. log N(y | mu, Q_xx) \\appox \n",
    "            hutchinson_mll = HutchinsonPseudoLoss(self, num_trace_samples=10)\n",
    "            return hutchinson_mll(mean, cov_mat, y)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown MLL approximation method: {self.mll_approx}\")\n",
    "        \n",
    "    # -----------------------------------------------------\n",
    "    # Fit\n",
    "    # -----------------------------------------------------\n",
    "    def _direct_solve_fit(self, M, N, X, y, K_zz):\n",
    "        # Construct A and b for linear solve\n",
    "        #   A = (K_zz + hat{K}_zx @ noise^{-1} @ hat{K}_xz)\n",
    "        #   b = (hat{K}_zx @ noise^{-1}) y\n",
    "        if X.shape[0] * X.shape[1] <= 32768:\n",
    "            # Case: \"small\" X\n",
    "            # Form estimate \\hat{K}_xz ~= W_xz K_zz\n",
    "            W_xz = self._interp(X)\n",
    "            hat_K_xz = W_xz @ K_zz\n",
    "            hat_K_zx = hat_K_xz.T\n",
    "            \n",
    "            # Form A and b\n",
    "            Lambda_inv_diag = (1 / self.noise) * torch.ones(N, dtype=self.dtype).to(self.device)\n",
    "            A = K_zz + hat_K_zx @ (Lambda_inv_diag.unsqueeze(1) * hat_K_xz)\n",
    "            b = hat_K_zx @ (Lambda_inv_diag * y)\n",
    "        else:\n",
    "            # Case: \"large\" X\n",
    "            with torch.no_grad():\n",
    "                # Initialize outputs\n",
    "                A = torch.zeros(M, M, dtype=self.dtype, device=self.device)\n",
    "                b = torch.zeros(M, dtype=self.dtype, device=self.device)\n",
    "                \n",
    "                # Initialize temporary values\n",
    "                fit_chunk_size = self.fit_chunk_size\n",
    "                batches = int(np.floor(N / fit_chunk_size))\n",
    "                Lambda_inv = (1 / self.noise) * torch.eye(fit_chunk_size, dtype=self.dtype, device=self.device)\n",
    "                tmp1 = torch.zeros(fit_chunk_size, M, dtype=self.dtype, device=self.device)\n",
    "                tmp2 = torch.zeros(M, M, dtype=self.dtype, device=self.device)\n",
    "                tmp3 = torch.zeros(fit_chunk_size, dtype=self.dtype, device=self.device)\n",
    "                tmp4 = torch.zeros(M, dtype=self.dtype, device=self.device)\n",
    "                tmp5 = torch.zeros(M, dtype=self.dtype, device=self.device)\n",
    "                \n",
    "                # Compute batches\n",
    "                for i in range(batches):\n",
    "                    # Update A: A += W_zx @ Lambda_inv @ W_xz\n",
    "                    X_batch = X[i*fit_chunk_size:(i+1)*fit_chunk_size]\n",
    "                    W_xz = self._interp(X_batch)\n",
    "                    W_zx = W_xz.T\n",
    "                    torch.matmul(Lambda_inv, W_xz, out=tmp1)\n",
    "                    torch.matmul(W_zx, tmp1, out=tmp2)\n",
    "                    A.add_(tmp2)\n",
    "                    \n",
    "                    # Update b: b += K_zz @ W_zx @ (Lambda_inv @ Y[i*batch_size:(i+1)*batch_size])\n",
    "                    torch.matmul(Lambda_inv, y[i*fit_chunk_size:(i+1)*fit_chunk_size], out=tmp3)\n",
    "                    torch.matmul(W_zx, tmp3, out=tmp4)\n",
    "                    torch.matmul(K_zz, tmp4, out=tmp5)\n",
    "                    b.add_(tmp5)\n",
    "                \n",
    "                # Compute last batch\n",
    "                if N - (i+1)*fit_chunk_size > 0:\n",
    "                    Lambda_inv = (1 / self.noise) * torch.eye(N - (i+1)*fit_chunk_size, dtype=self.dtype, device=self.device)\n",
    "                    X_batch = X[(i+1)*fit_chunk_size:]\n",
    "                    W_xz = self._interp(X_batch)\n",
    "                    A += W_xz.T @ Lambda_inv @ W_xz\n",
    "                    b += K_zz @ W_xz.T @ Lambda_inv @ y[(i+1)*fit_chunk_size:]\n",
    "\n",
    "                # Aggregate result\n",
    "                A = K_zz + K_zz @ A @ K_zz\n",
    "\n",
    "        # Safe solve A \\alpha = b\n",
    "        A = DenseLinearOperator(A)\n",
    "        self.alpha, use_pinv = self._solve_system(\n",
    "            A,\n",
    "            b.unsqueeze(1),\n",
    "            x0=torch.zeros_like(b),\n",
    "            forwards_matmul=A.matmul,\n",
    "            precond=None,\n",
    "            return_pinv=True\n",
    "        )\n",
    "\n",
    "        # Store for fast prediction\n",
    "        self.K_zz_alpha = K_zz @ self.alpha\n",
    "        return use_pinv\n",
    "    # def _direct_solve_fit(self, M, N, X, y, K_zz):\n",
    "    #     # Construct A and b for linear solve\n",
    "    #     #   A = (K_zz + hat{K}_zx @ noise^{-1} @ hat{K}_xz)\n",
    "    #     #   b = (hat{K}_zx @ noise^{-1}) y\n",
    "    #     if X.shape[0] * X.shape[1] <= 32768:\n",
    "    #         # Case: \"small\" X\n",
    "    #         # Form estimate \\hat{K}_xz ~= W_xz K_zz\n",
    "    #         W_xz = self._interp(X)\n",
    "    #         hat_K_xz = W_xz @ K_zz\n",
    "    #         hat_K_zx = hat_K_xz.T\n",
    "            \n",
    "    #         # Form A and b\n",
    "    #         Lambda_inv_diag = (1 / self.noise) * torch.ones(N, dtype=self.dtype).to(self.device)\n",
    "    #         A = K_zz + hat_K_zx @ (Lambda_inv_diag.unsqueeze(1) * hat_K_xz)\n",
    "    #         b = hat_K_zx @ (Lambda_inv_diag * y)\n",
    "    #     else:\n",
    "    #         # Case: \"large\" X\n",
    "    #         with torch.no_grad():\n",
    "    #             # Initialize outputs\n",
    "    #             A = torch.zeros(M, M, dtype=self.dtype, device=self.device)\n",
    "    #             b = torch.zeros(M, dtype=self.dtype, device=self.device)\n",
    "                \n",
    "    #             # Initialize temporary values\n",
    "    #             fit_chunk_size = self.fit_chunk_size\n",
    "    #             batches = int(np.floor(N / fit_chunk_size))\n",
    "    #             Lambda_inv = (1 / self.noise) * torch.eye(fit_chunk_size, dtype=self.dtype, device=self.device)\n",
    "    #             tmp1 = torch.zeros(fit_chunk_size, M, dtype=self.dtype, device=self.device)\n",
    "    #             tmp2 = torch.zeros(M, M, dtype=self.dtype, device=self.device)\n",
    "    #             tmp3 = torch.zeros(fit_chunk_size, dtype=self.dtype, device=self.device)\n",
    "    #             tmp4 = torch.zeros(M, dtype=self.dtype, device=self.device)\n",
    "    #             tmp5 = torch.zeros(M, dtype=self.dtype, device=self.device)\n",
    "                \n",
    "    #         for i in range(batches):\n",
    "    #             # Update A: A += W_zx @ Lambda_inv @ W_xz\n",
    "    #             X_batch = X[i*fit_chunk_size:(i+1)*fit_chunk_size]\n",
    "                \n",
    "    #             # Compute W_xz as sparse tensor\n",
    "    #             W_xz = self._interp(X_batch)  # This returns a sparse tensor\n",
    "    #             W_zx = W_xz.T# Transpose to get W_zx\n",
    "    #             sparsity = 1.0 - W_xz._nnz() / float(W_xz.numel())\n",
    "    #             print(sparsity)\n",
    "    #             # Perform sparse-dense matrix multiplication\n",
    "    #             tmp1 = torch.sparse.mm(Lambda_inv,W_xz)  # Sparse x Dense\n",
    "    #             tmp2 = torch.sparse.mm(W_zx, tmp1)  # Sparse x Dense\n",
    "    #             A.add_(tmp2)  # Update A\n",
    "                \n",
    "    #             # Update b: b += K_zz @ W_zx @ (Lambda_inv @ Y_batch)\n",
    "    #             Y_batch = y[i*fit_chunk_size:(i+1)*fit_chunk_size]\n",
    "    #             tmp3 = torch.matmul(Lambda_inv, Y_batch)  # Dense x Dense\n",
    "    #             tmp4 = torch.sparse.mm(W_zx, tmp3.unsqueeze(-1) )  # Sparse x Dense\n",
    "    #             tmp5 = torch.matmul(K_zz, tmp4)  # Dense x Dense\n",
    "    #             b.add_(tmp5.squeeze())\n",
    "                \n",
    "    #             # Compute last batch\n",
    "    #         if N - (i + 1) * fit_chunk_size > 0:\n",
    "    #             last_batch_size = N - (i + 1) * fit_chunk_size\n",
    "                \n",
    "    #             # Create the identity matrix for Lambda_inv for the last batch\n",
    "    #             Lambda_inv = (1 / self.noise) * torch.eye(last_batch_size, dtype=self.dtype, device=self.device)\n",
    "                \n",
    "    #             # Get the last batch of X and y\n",
    "    #             X_batch = X[(i + 1) * fit_chunk_size:]\n",
    "    #             Y_batch = y[(i + 1) * fit_chunk_size:]\n",
    "                \n",
    "    #             # Compute W_xz as a sparse tensor for the last batch\n",
    "    #             W_xz = self._interp(X_batch)\n",
    "                \n",
    "    #             # Transpose the sparse tensor W_xz to get W_zx\n",
    "    #             W_zx = W_xz.transpose(0, 1)\n",
    "    #             # Update A: A += W_zx @ Lambda_inv @ W_xz\n",
    "    #             tmp1 = torch.sparse.mm( Lambda_inv,W_xz)  # Sparse x Dense\n",
    "    #             tmp2 = torch.sparse.mm(W_zx, tmp1)  # Sparse x Dense\n",
    "    #             A.add_(tmp2)  # Update A (dense)\n",
    "                \n",
    "    #             # Update b: b += K_zz @ W_zx @ (Lambda_inv @ Y_batch)\n",
    "    #             tmp3 = torch.matmul(Lambda_inv, Y_batch)  # Dense x Dense\n",
    "    #             tmp4 = torch.sparse.mm(W_zx, tmp3.unsqueeze(-1))  # Sparse x Dense\n",
    "    #             tmp5 = torch.matmul(K_zz, tmp4)  # Dense x Dense\n",
    "    #             b.add_(tmp5.squeeze())  # Update b\n",
    "\n",
    "\n",
    "\n",
    "    #         # Aggregate result\n",
    "    #         A = K_zz + K_zz @ A @ K_zz\n",
    "\n",
    "    #     # Safe solve A \\alpha = b\n",
    "    #     A = DenseLinearOperator(A)\n",
    "    #     self.alpha, use_pinv = self._solve_system(\n",
    "    #         A,\n",
    "    #         b.unsqueeze(1),\n",
    "    #         x0=torch.zeros_like(b),\n",
    "    #         forwards_matmul=A.matmul,\n",
    "    #         precond=None,\n",
    "    #         return_pinv=True\n",
    "    #     )\n",
    "\n",
    "    #     # Store for fast prediction\n",
    "    #     self.K_zz_alpha = K_zz @ self.alpha\n",
    "    #     return use_pinv\n",
    "\n",
    "    def _qr_solve_fit(self, M, N, X, y, K_zz):\n",
    "        if X.shape[0] * X.shape[1] <= 32768:\n",
    "            # Compute: W_xz K_zz\n",
    "            print(\"USING QR SMALL\")\n",
    "            W_xz = self._interp(X)\n",
    "            hat_K_xz = W_xz @ K_zz\n",
    "        else:\n",
    "            # Compute: W_xz K_zz in a batched fashion\n",
    "            print(\"USING QR BATCH\")\n",
    "            with torch.no_grad():\n",
    "                # Compute batches\n",
    "                fit_chunk_size = self.fit_chunk_size\n",
    "                batches = int(np.floor(N / fit_chunk_size))\n",
    "                Lambda_half_inv_diag = (1 / torch.sqrt(self.noise)) * torch.ones(fit_chunk_size, dtype=self.dtype, device=self.device)\n",
    "                hat_K_xz = torch.zeros((N, M), dtype=self.dtype, device=self.device)\n",
    "                for i in range(batches):\n",
    "                    start = i*fit_chunk_size\n",
    "                    end = (i+1)*fit_chunk_size\n",
    "                    X_batch = X[start:end,:]\n",
    "                    W_xz = self._interp(X_batch)\n",
    "                    torch.matmul(W_xz, K_zz, out=hat_K_xz[start:end,:])\n",
    "                \n",
    "                start = (i+1)*fit_chunk_size\n",
    "                if N - start > 0:\n",
    "                    Lambda_half_inv_diag = (1 / torch.sqrt(self.noise)) * torch.eye(N - (i+1)*fit_chunk_size, dtype=self.dtype, device=self.device)\n",
    "                    X_batch = X[start:]\n",
    "                    W_xz = self._interp(X_batch)\n",
    "                    torch.matmul(W_xz, K_zz, out=hat_K_xz[start:,:])\n",
    "        \n",
    "        # B^T = [(Lambda^{-1/2} \\hat{K}_xz) U_zz ]\n",
    "        U_zz = psd_safe_cholesky(K_zz, upper=True, max_tries=10)\n",
    "        Lambda_half_inv_diag = (1 / torch.sqrt(self.noise)) * torch.ones(N, dtype=self.dtype).to(self.device)\n",
    "        B = torch.cat([Lambda_half_inv_diag.unsqueeze(1) * hat_K_xz, U_zz], dim=0)\n",
    "\n",
    "        # B = QR\n",
    "        Q, R = torch.linalg.qr(B)\n",
    "\n",
    "        # \\alpha = R^{-1} @ Q^T @ Lambda^{-1/2}b\n",
    "        b = Lambda_half_inv_diag * y\n",
    "        self.alpha = torch.linalg.solve_triangular(R, (Q.T[:, 0:N] @ b).unsqueeze(1), upper=True).squeeze(1) # (should use triangular solve)\n",
    "        # self.alpha = ((torch.linalg.inv(R) @ Q.T)[:, :N] @ b)\n",
    "        \n",
    "        # Store for fast inference\n",
    "        self.K_zz_alpha = K_zz @ self.alpha\n",
    "\n",
    "        return False\n",
    "\n",
    "    def fit(self, X: torch.Tensor, y: torch.Tensor) -> bool:\n",
    "        \"\"\"Fits a SoftGP to dataset (X, y). That is, solve:\n",
    "\n",
    "                (hat{K}_zx @ noise^{-1}) y = (K_zz + hat{K}_zx @ noise^{-1} @ hat{K}_xz) \\alpha\n",
    "        \n",
    "            for \\alpha where\n",
    "            1. inducing points z are fixed,\n",
    "            2. hat{K}_zx = K_zz W_zx, and\n",
    "            3. hat{K}_xz = hat{K}_zx^T.\n",
    "\n",
    "        Args:\n",
    "            X (torch.Tensor): N x D tensor of inputs\n",
    "            y (torch.Tensor): N tensor of outputs\n",
    "\n",
    "        Returns:\n",
    "            bool: Returns true if the pseudoinverse was used, false otherwise.\n",
    "        \"\"\"        \n",
    "        # Prepare inputs\n",
    "        N = len(X)\n",
    "        M = len(self.inducing_points)\n",
    "        X = X.to(self.device, dtype=self.dtype)\n",
    "        y = y.to(self.device, dtype=self.dtype)\n",
    "\n",
    "        # Form K_zz\n",
    "        K_zz = self._mk_cov(self.inducing_points)\n",
    "\n",
    "        if self.use_qr:\n",
    "            return self._qr_solve_fit(M, N, X, y, K_zz)\n",
    "        else:\n",
    "            return self._direct_solve_fit(M, N, X, y, K_zz)\n",
    "\n",
    "    # -----------------------------------------------------\n",
    "    # Predict\n",
    "    # -----------------------------------------------------\n",
    "\n",
    "    def pred(self, x_star: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Give the posterior predictive:\n",
    "        \n",
    "            p(y_star | x_star, X, y) \n",
    "                = W_star_z (K_zz \\alpha)\n",
    "                = W_star_z K_zz (K_zz + hat{K}_zx @ noise^{-1} @ hat{K}_xz)^{-1} (hat{K}_zx @ noise^{-1}) y\n",
    "\n",
    "        Args:\n",
    "            x_star (torch.Tensor): B x D tensor of points to evaluate at.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: B tensor of p(y_star | x_star, X, y).\n",
    "        \"\"\"        \n",
    "        W_star_z = self._interp(x_star)\n",
    "        return torch.matmul(W_star_z, self.K_zz_alpha).squeeze(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test version profiling stats\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from line_profiler import LineProfiler\n",
    "\n",
    "def eval_gp(model, test_dataset: Dataset, device=\"cuda:0\") -> float:\n",
    "    preds = []\n",
    "    neg_mlls = []\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False,num_workers=1)\n",
    "    for x_batch, y_batch in tqdm(test_loader):\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        preds += [(model.pred(x_batch) - y_batch).detach().cpu()**2]\n",
    "        neg_mlls += [-model.mll(x_batch, y_batch).detach().cpu()]\n",
    "    rmse = torch.sqrt(torch.sum(torch.cat(preds)) / len(test_dataset)).item()\n",
    "    neg_mll = torch.sum(torch.tensor(neg_mlls))\n",
    "            \n",
    "    print(\"RMSE:\", rmse, \"NEG_MLL\", neg_mll.item(), \"NOISE\", model.noise.cpu().item(), \"LENGTHSCALE\", model.get_lengthscale(), \"OUTPUTSCALE\", model.get_outputscale())\n",
    "    \n",
    "    return {\n",
    "        \"rmse\": rmse,\n",
    "        \"nll\": neg_mll,\n",
    "    }   \n",
    "    \n",
    "def profileGP(GP,train_dataset, test_dataset):\n",
    "    num_inducing = 512\n",
    "    dtype = torch.float32\n",
    "    batch_size = 1024\n",
    "    epochs = 2\n",
    "    lr = 0.01 \n",
    "    learn_noise = False\n",
    "    device = \"cpu\"\n",
    "    \n",
    "    # Initialize inducing points with kmeans\n",
    "    train_features, train_labels = flatten_dataset(train_dataset)\n",
    "    kmeans = KMeans(n_clusters=num_inducing)\n",
    "    kmeans.fit(train_features)\n",
    "    centers = kmeans.cluster_centers_\n",
    "    inducing_points = torch.tensor(centers).to(dtype=dtype, device=device)\n",
    "    \n",
    "    # Setup model\n",
    "    kernel = RBFKernel().to(device=device, dtype=dtype)\n",
    "\n",
    "    model = GP(kernel,\n",
    "        inducing_points,\n",
    "        noise=1e-3,\n",
    "        learn_noise=learn_noise,\n",
    "        use_scale=False,\n",
    "        dtype=dtype,\n",
    "        solver=\"solve\",\n",
    "        max_cg_iter=50,\n",
    "        cg_tolerance=0.5,\n",
    "        mll_approx=\"hutchinson\",\n",
    "        fit_chunk_size=1024,\n",
    "        use_qr=True,\n",
    "    )\n",
    "\n",
    "    if learn_noise:\n",
    "        params = model.parameters()\n",
    "    else:\n",
    "        params = filter_param(model.named_parameters(), \"likelihood.noise_covar.raw_noise\")\n",
    "    optimizer = torch.optim.Adam(params, lr=lr)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    pbar = tqdm(range(epochs), desc=\"Optimizing MLL\")\n",
    "    def train_model():\n",
    "        for epoch in pbar:\n",
    "            t1 = time.perf_counter()\n",
    "            \n",
    "            neg_mlls = []\n",
    "            for x_batch, y_batch in train_loader:\n",
    "                x_batch = x_batch.clone().detach().to(dtype=dtype, device=device)\n",
    "                y_batch = y_batch.clone().detach().to(dtype=dtype, device=device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                with gpytorch.settings.max_root_decomposition_size(100), max_cholesky_size(int(1.e7)):\n",
    "                    neg_mll = -model.mll(x_batch, y_batch)\n",
    "                neg_mlls += [-neg_mll.item()]\n",
    "                neg_mll.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                pbar.set_description(f\"Epoch {epoch+1}/{epochs}\")\n",
    "                pbar.set_postfix(MLL=f\"{-neg_mll.item()}\")\n",
    "            t2 = time.perf_counter()\n",
    "\n",
    "            use_pinv = model.fit(train_features, train_labels)\n",
    "            t3 = time.perf_counter()\n",
    "            #results = eval_gp(model, test_dataset, device=device)\n",
    "            #print(results)\n",
    "    profiler = LineProfiler()\n",
    "    profiler.add_function(train_model)\n",
    "    profiler.add_function(model.fit)  \n",
    "    profiler.add_function(model.interp)  \n",
    "    profiler.add_function(model._qr_solve_fit) \n",
    "    profiler.add_function(model.mll)  \n",
    "\n",
    "\n",
    "    profiler.enable_by_count()\n",
    "    train_model() \n",
    "    profiler.disable_by_count()\n",
    "    profiler.print_stats()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "- Total time: 76.4511 s QR no changes \n",
    "\n",
    "- Total time: 46.756 s sparse \n",
    "\n",
    "- baseline + boltz mask\n",
    "- Total time: 17.1253 s, 15.5868 s, 14.7804 s\n",
    "\n",
    "\n",
    "baseline no mask \n",
    "Total time  22.1084 s  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIZE (15000, 27)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:01<00:00,  1.89it/s]\n",
      "c:\\Users\\chris\\anaconda3\\envs\\softgp\\Lib\\site-packages\\threadpoolctl.py:1214: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "Epoch 1/2:   0%|          | 0/2 [00:16<?, ?it/s, MLL=49.041358947753906]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING QR BATCH\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2:  50%|█████     | 1/2 [00:39<00:21, 21.99s/it, MLL=74.28733825683594] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING QR BATCH\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2: 100%|██████████| 2/2 [00:44<00:00, 22.34s/it, MLL=74.28733825683594]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-07 s\n",
      "\n",
      "Total time: 44.6828 s\n",
      "File: C:\\Users\\chris\\AppData\\Local\\Temp\\ipykernel_29896\\3236570565.py\n",
      "Function: train_model at line 63\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "    63                                               def train_model():\n",
      "    64         3      39386.0  13128.7      0.0          for epoch in pbar:\n",
      "    65         2         79.0     39.5      0.0              t1 = time.perf_counter()\n",
      "    66                                                       \n",
      "    67         2         22.0     11.0      0.0              neg_mlls = []\n",
      "    68        30   85255323.0    3e+06     19.1              for x_batch, y_batch in train_loader:\n",
      "    69        28      36040.0   1287.1      0.0                  x_batch = x_batch.clone().detach().to(dtype=dtype, device=device)\n",
      "    70        28       2243.0     80.1      0.0                  y_batch = y_batch.clone().detach().to(dtype=dtype, device=device)\n",
      "    71                                           \n",
      "    72        28      42251.0   1509.0      0.0                  optimizer.zero_grad()\n",
      "    73        56      10119.0    180.7      0.0                  with gpytorch.settings.max_root_decomposition_size(100), max_cholesky_size(int(1.e7)):\n",
      "    74        28  113767376.0    4e+06     25.5                      neg_mll = -model.mll(x_batch, y_batch)\n",
      "    75        28       2801.0    100.0      0.0                  neg_mlls += [-neg_mll.item()]\n",
      "    76        28  138095645.0    5e+06     30.9                  neg_mll.backward()\n",
      "    77        28     263658.0   9416.4      0.1                  optimizer.step()\n",
      "    78                                           \n",
      "    79        28     374229.0  13365.3      0.1                  pbar.set_description(f\"Epoch {epoch+1}/{epochs}\")\n",
      "    80        28     317316.0  11332.7      0.1                  pbar.set_postfix(MLL=f\"{-neg_mll.item()}\")\n",
      "    81         2         37.0     18.5      0.0              t2 = time.perf_counter()\n",
      "    82                                           \n",
      "    83         2  108621478.0    5e+07     24.3              use_pinv = model.fit(train_features, train_labels)\n",
      "    84         2        239.0    119.5      0.0              t3 = time.perf_counter()\n",
      "    85                                                       #results = eval_gp(model, test_dataset, device=device)\n",
      "    86                                                       #print(results)\n",
      "\n",
      "Total time: 1.4663 s\n",
      "File: C:\\Users\\chris\\AppData\\Local\\Temp\\ipykernel_29896\\427513435.py\n",
      "Function: softmax_interp at line 69\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "    69                                                   def softmax_interp( X: torch.Tensor, sigma_values: torch.Tensor) -> torch.Tensor:\n",
      "    70        56    4340641.0  77511.4     29.6              X = X / .001 # Use the learnable T\n",
      "    71        56    8104102.0 144716.1     55.3              distances = torch.linalg.vector_norm(X - sigma_values, ord=2, dim=-1)\n",
      "    72        56    1505708.0  26887.6     10.3              softmax_distances = torch.softmax(-distances, dim=-1)\n",
      "    73        56     711136.0  12698.9      4.8              softmax_distances = torch.where(softmax_distances < 1e-16, torch.tensor(0.0, dtype=softmax_distances.dtype), softmax_distances)\n",
      "    74                                                       \n",
      "    75        56       1426.0     25.5      0.0              return softmax_distances\n",
      "\n",
      "Total time: 11.3742 s\n",
      "File: C:\\Users\\chris\\AppData\\Local\\Temp\\ipykernel_29896\\427513435.py\n",
      "Function: mll at line 178\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   178                                               def mll(self, X: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
      "   179                                                   \"\"\"Compute the marginal log likelihood of a soft GP:\n",
      "   180                                                       \n",
      "   181                                                       log p(y) = log N(y | mu_x, Q_xx)\n",
      "   182                                           \n",
      "   183                                                       where\n",
      "   184                                                           mu_X: mean of soft GP\n",
      "   185                                                           Q_XX = W_xz K_zz W_zx\n",
      "   186                                           \n",
      "   187                                                   Args:\n",
      "   188                                                       X (torch.Tensor): B x D tensor of inputs where each row is a point.\n",
      "   189                                                       y (torch.Tensor): B tensor of targets.\n",
      "   190                                           \n",
      "   191                                                   Returns:\n",
      "   192                                                       torch.Tensor:  log p(y)\n",
      "   193                                                   \"\"\"        \n",
      "   194                                                   # Construct covariance matrix components\n",
      "   195        28    1312770.0  46884.6      1.2          K_zz = self._mk_cov(self.inducing_points)\n",
      "   196        28    7202528.0 257233.1      6.3          W_xz = self._interp(X)\n",
      "   197                                                   \n",
      "   198        28        943.0     33.7      0.0          if self.mll_approx == \"exact\":\n",
      "   199                                                       # [Note]: Compute MLL with a multivariate normal. Unstable for float.\n",
      "   200                                                       # 1. mean: 0\n",
      "   201                                                       mean = torch.zeros(len(X), dtype=self.dtype, device=self.device)\n",
      "   202                                                       \n",
      "   203                                                       # 2. covariance: Q_xx = (W_xz L) (L^T W_xz) + noise I  where K_zz = L L^T\n",
      "   204                                                       L = psd_safe_cholesky(K_zz)\n",
      "   205                                                       LK = (W_xz @ L).to(device=self.device)\n",
      "   206                                                       cov_diag = self.noise * torch.ones(len(X), dtype=self.dtype, device=self.device)\n",
      "   207                                           \n",
      "   208                                                       # 3. N(mu, Q_xx)\n",
      "   209                                                       normal_dist = torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal(mean, LK, cov_diag, validate_args=None)\n",
      "   210                                                       \n",
      "   211                                                       # 4. log N(y | mu, Q_xx)\n",
      "   212                                                       return normal_dist.log_prob(y)\n",
      "   213        28        299.0     10.7      0.0          elif self.mll_approx == \"hutchinson\":\n",
      "   214                                                       # [Note]: Compute MLL with Hutchinson's trace estimator\n",
      "   215                                                       # 1. mean: 0\n",
      "   216        28      15469.0    552.5      0.0              mean = torch.zeros(len(X), dtype=self.dtype, device=self.device)\n",
      "   217                                                       \n",
      "   218                                                       # 2. covariance: Q_xx = W_xz K_zz K_zx + noise I\n",
      "   219        28   91467239.0    3e+06     80.4              cov_mat = W_xz @ K_zz @ W_xz.T \n",
      "   220        28    1298320.0  46368.6      1.1              cov_mat += torch.eye(cov_mat.shape[1], dtype=self.dtype, device=self.device) * self.noise\n",
      "   221                                           \n",
      "   222                                                       # 3. log N(y | mu, Q_xx) \\appox \n",
      "   223        28       3476.0    124.1      0.0              hutchinson_mll = HutchinsonPseudoLoss(self, num_trace_samples=10)\n",
      "   224        28   12441250.0 444330.4     10.9              return hutchinson_mll(mean, cov_mat, y)\n",
      "   225                                                   else:\n",
      "   226                                                       raise ValueError(f\"Unknown MLL approximation method: {self.mll_approx}\")\n",
      "\n",
      "Total time: 10.8507 s\n",
      "File: C:\\Users\\chris\\AppData\\Local\\Temp\\ipykernel_29896\\427513435.py\n",
      "Function: _qr_solve_fit at line 404\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   404                                               def _qr_solve_fit(self, M, N, X, y, K_zz):\n",
      "   405         2        202.0    101.0      0.0          if X.shape[0] * X.shape[1] <= 32768:\n",
      "   406                                                       # Compute: W_xz K_zz\n",
      "   407                                                       print(\"USING QR SMALL\")\n",
      "   408                                                       W_xz = self._interp(X)\n",
      "   409                                                       hat_K_xz = W_xz @ K_zz\n",
      "   410                                                   else:\n",
      "   411                                                       # Compute: W_xz K_zz in a batched fashion\n",
      "   412         2       2102.0   1051.0      0.0              print(\"USING QR BATCH\")\n",
      "   413         4       1525.0    381.2      0.0              with torch.no_grad():\n",
      "   414                                                           # Compute batches\n",
      "   415         2         21.0     10.5      0.0                  fit_chunk_size = self.fit_chunk_size\n",
      "   416         2        491.0    245.5      0.0                  batches = int(np.floor(N / fit_chunk_size))\n",
      "   417         2      41383.0  20691.5      0.0                  Lambda_half_inv_diag = (1 / torch.sqrt(self.noise)) * torch.ones(fit_chunk_size, dtype=self.dtype, device=self.device)\n",
      "   418         2      84894.0  42447.0      0.1                  hat_K_xz = torch.zeros((N, M), dtype=self.dtype, device=self.device)\n",
      "   419        28        720.0     25.7      0.0                  for i in range(batches):\n",
      "   420        26        275.0     10.6      0.0                      start = i*fit_chunk_size\n",
      "   421        26        314.0     12.1      0.0                      end = (i+1)*fit_chunk_size\n",
      "   422        26      11644.0    447.8      0.0                      X_batch = X[start:end,:]\n",
      "   423        26    7349447.0 282671.0      6.8                      W_xz = self._interp(X_batch)\n",
      "   424        26   90249913.0    3e+06     83.2                      torch.matmul(W_xz, K_zz, out=hat_K_xz[start:end,:])\n",
      "   425                                                           \n",
      "   426         2         52.0     26.0      0.0                  start = (i+1)*fit_chunk_size\n",
      "   427         2         29.0     14.5      0.0                  if N - start > 0:\n",
      "   428         2       4530.0   2265.0      0.0                      Lambda_half_inv_diag = (1 / torch.sqrt(self.noise)) * torch.eye(N - (i+1)*fit_chunk_size, dtype=self.dtype, device=self.device)\n",
      "   429         2        388.0    194.0      0.0                      X_batch = X[start:]\n",
      "   430         2     156007.0  78003.5      0.1                      W_xz = self._interp(X_batch)\n",
      "   431         2    1319695.0 659847.5      1.2                      torch.matmul(W_xz, K_zz, out=hat_K_xz[start:,:])\n",
      "   432                                                   \n",
      "   433                                                   # B^T = [(Lambda^{-1/2} \\hat{K}_xz) U_zz ]\n",
      "   434         2     611769.0 305884.5      0.6          U_zz = psd_safe_cholesky(K_zz, upper=True, max_tries=10)\n",
      "   435         2       3929.0   1964.5      0.0          Lambda_half_inv_diag = (1 / torch.sqrt(self.noise)) * torch.ones(N, dtype=self.dtype).to(self.device)\n",
      "   436         2     363318.0 181659.0      0.3          B = torch.cat([Lambda_half_inv_diag.unsqueeze(1) * hat_K_xz, U_zz], dim=0)\n",
      "   437                                           \n",
      "   438                                                   # B = QR\n",
      "   439         2    8211294.0    4e+06      7.6          Q, R = torch.linalg.qr(B)\n",
      "   440                                           \n",
      "   441                                                   # \\alpha = R^{-1} @ Q^T @ Lambda^{-1/2}b\n",
      "   442         2        821.0    410.5      0.0          b = Lambda_half_inv_diag * y\n",
      "   443         2      59079.0  29539.5      0.1          self.alpha = torch.linalg.solve_triangular(R, (Q.T[:, 0:N] @ b).unsqueeze(1), upper=True).squeeze(1) # (should use triangular solve)\n",
      "   444                                                   # self.alpha = ((torch.linalg.inv(R) @ Q.T)[:, :N] @ b)\n",
      "   445                                                   \n",
      "   446                                                   # Store for fast inference\n",
      "   447         2      33412.0  16706.0      0.0          self.K_zz_alpha = K_zz @ self.alpha\n",
      "   448                                           \n",
      "   449         2         85.0     42.5      0.0          return False\n",
      "\n",
      "Total time: 10.8621 s\n",
      "File: C:\\Users\\chris\\AppData\\Local\\Temp\\ipykernel_29896\\427513435.py\n",
      "Function: fit at line 451\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   451                                               def fit(self, X: torch.Tensor, y: torch.Tensor) -> bool:\n",
      "   452                                                   \"\"\"Fits a SoftGP to dataset (X, y). That is, solve:\n",
      "   453                                           \n",
      "   454                                                           (hat{K}_zx @ noise^{-1}) y = (K_zz + hat{K}_zx @ noise^{-1} @ hat{K}_xz) \\alpha\n",
      "   455                                                   \n",
      "   456                                                       for \\alpha where\n",
      "   457                                                       1. inducing points z are fixed,\n",
      "   458                                                       2. hat{K}_zx = K_zz W_zx, and\n",
      "   459                                                       3. hat{K}_xz = hat{K}_zx^T.\n",
      "   460                                           \n",
      "   461                                                   Args:\n",
      "   462                                                       X (torch.Tensor): N x D tensor of inputs\n",
      "   463                                                       y (torch.Tensor): N tensor of outputs\n",
      "   464                                           \n",
      "   465                                                   Returns:\n",
      "   466                                                       bool: Returns true if the pseudoinverse was used, false otherwise.\n",
      "   467                                                   \"\"\"        \n",
      "   468                                                   # Prepare inputs\n",
      "   469         2        378.0    189.0      0.0          N = len(X)\n",
      "   470         2        177.0     88.5      0.0          M = len(self.inducing_points)\n",
      "   471         2        270.0    135.0      0.0          X = X.to(self.device, dtype=self.dtype)\n",
      "   472         2         30.0     15.0      0.0          y = y.to(self.device, dtype=self.dtype)\n",
      "   473                                           \n",
      "   474                                                   # Form K_zz\n",
      "   475         2      92001.0  46000.5      0.1          K_zz = self._mk_cov(self.inducing_points)\n",
      "   476                                           \n",
      "   477         2         35.0     17.5      0.0          if self.use_qr:\n",
      "   478         2  108528127.0    5e+07     99.9              return self._qr_solve_fit(M, N, X, y, K_zz)\n",
      "   479                                                   else:\n",
      "   480                                                       return self._direct_solve_fit(M, N, X, y, K_zz)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from data.get_uci import ElevatorsDataset\n",
    "dataset = ElevatorsDataset(\"../data/uci_datasets/uci_datasets/pol/data.csv\")\n",
    "train_dataset, val_dataset, test_dataset = split_dataset(\n",
    "    dataset,\n",
    "    train_frac=9/10,\n",
    "    val_frac=0/10\n",
    ")\n",
    "\n",
    "softgp = profileGP(SoftGP_test,train_dataset,test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison: Test vs Baseline \n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIZE (15000, 27)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  2.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SoftGP_baseline...\n",
      "USING QR BATCH\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [00:05<00:00,  8.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.4898330271244049 NEG_MLL -1650.2716064453125 NOISE 0.0010000000474974513 LENGTHSCALE tensor([[0.6957]], grad_fn=<SoftplusBackward0>) OUTPUTSCALE 1.0\n",
      "USING QR BATCH\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [00:06<00:00,  7.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.45824193954467773 NEG_MLL -1488.86328125 NOISE 0.0010000000474974513 LENGTHSCALE tensor([[0.6755]], grad_fn=<SoftplusBackward0>) OUTPUTSCALE 1.0\n",
      "USING QR BATCH\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [00:05<00:00,  9.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.43424949049949646 NEG_MLL -1380.808349609375 NOISE 0.0010000000474974513 LENGTHSCALE tensor([[0.6842]], grad_fn=<SoftplusBackward0>) OUTPUTSCALE 1.0\n",
      "USING QR BATCH\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/47 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "def plot_results(GP_classes, all_mean_rmse, all_mean_runtimes, all_std_rmse, all_std_runtimes, epochs):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "    for i, GP_class in enumerate(GP_classes):\n",
    "        epochs_range = range(1, epochs + 1)\n",
    "        axes[0].plot(epochs_range, all_mean_rmse[i], label=f'{GP_class.__name__} RMSE')\n",
    "        axes[0].fill_between(epochs_range,\n",
    "                             [m - s for m, s in zip(all_mean_rmse[i], all_std_rmse[i])],\n",
    "                             [m + s for m, s in zip(all_mean_rmse[i], all_std_rmse[i])],\n",
    "                             alpha=0.3)\n",
    "    axes[0].set_title('RMSE per Epoch')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('RMSE')\n",
    "    axes[0].legend()\n",
    "\n",
    "    for i, GP_class in enumerate(GP_classes):\n",
    "        epochs_range = range(1, epochs + 1)\n",
    "        axes[1].plot(epochs_range, all_mean_runtimes[i], label=f'{GP_class.__name__} Runtime')\n",
    "        axes[1].fill_between(epochs_range,\n",
    "                             [m - s for m, s in zip(all_mean_runtimes[i], all_std_runtimes[i])],\n",
    "                             [m + s for m, s in zip(all_mean_runtimes[i], all_std_runtimes[i])],\n",
    "                             alpha=0.3)\n",
    "    axes[1].set_title('Training Time per Epoch')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Time (s)')\n",
    "    axes[1].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def train_gp(GP_class, inducing_points, test_dataset, train_features, train_labels, epochs, device, dtype):\n",
    "    kernel = RBFKernel().to(device=device, dtype=dtype)\n",
    "    learn_noise = False\n",
    "    lr = .01\n",
    "    batch_size = 1024\n",
    "\n",
    "    model = GP_class(kernel,\n",
    "                     inducing_points,\n",
    "                     noise=1e-3,\n",
    "                     learn_noise=learn_noise,\n",
    "                     use_scale=False,\n",
    "                     dtype=dtype,\n",
    "                     solver=\"solve\",\n",
    "                     max_cg_iter=50,\n",
    "                     cg_tolerance=0.5,\n",
    "                     mll_approx=\"hutchinson\",\n",
    "                     fit_chunk_size=1024,\n",
    "                     use_qr=True)\n",
    "\n",
    "    epoch_runtimes = []\n",
    "    epoch_rmse = []\n",
    "\n",
    "    # pbar = tqdm(range(epochs), desc=\"Optimizing MLL\")\n",
    "    if learn_noise:\n",
    "        params = model.parameters()\n",
    "    else:\n",
    "        params = filter_param(model.named_parameters(), \"likelihood.noise_covar.raw_noise\")\n",
    "    optimizer = torch.optim.Adam(params, lr=lr)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    def train_model():\n",
    "        #==================Train============================\n",
    "        for _ in range(epochs):\n",
    "            epoch_start_time = time.time()\n",
    "            for x_batch, y_batch in train_loader:\n",
    "                x_batch = x_batch.clone().detach().to(dtype=dtype, device=device)\n",
    "                y_batch = y_batch.clone().detach().to(dtype=dtype, device=device)\n",
    "                optimizer.zero_grad()\n",
    "                with gpytorch.settings.max_root_decomposition_size(100), max_cholesky_size(int(1.e7)):\n",
    "                    neg_mll = -model.mll(x_batch, y_batch)\n",
    "                neg_mll.backward()\n",
    "                optimizer.step()\n",
    "                # pbar.set_description(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "                # pbar.set_postfix(MLL=f\"{-neg_mll.item()}\")\n",
    "\n",
    "            model.fit(train_features, train_labels)\n",
    "            epoch_end_time = time.time()\n",
    "            epoch_runtimes.append(epoch_end_time - epoch_start_time)\n",
    "\n",
    "            #==================Evaluate============================\n",
    "            eval_results = eval_gp(model, test_dataset, device=device)\n",
    "            epoch_rmse.append(eval_results['rmse'])\n",
    "    \n",
    "    train_model()\n",
    "    return epoch_rmse, epoch_runtimes\n",
    "\n",
    "def benchmark(GP_classes, train_dataset, test_dataset, epochs=2, seed=42, N=3):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "    num_inducing = 512\n",
    "    dtype = torch.float32\n",
    "    device = \"cpu\"\n",
    "    \n",
    "    all_mean_rmse = []\n",
    "    all_mean_runtimes = []\n",
    "    all_std_rmse = []\n",
    "    all_std_runtimes = []\n",
    "\n",
    "    #==================Inducing Points============================\n",
    "    train_features, train_labels = flatten_dataset(train_dataset)\n",
    "    kmeans = KMeans(n_clusters=num_inducing)\n",
    "    kmeans.fit(train_features)\n",
    "    centers = kmeans.cluster_centers_\n",
    "    inducing_points = torch.tensor(centers).to(dtype=dtype, device=device)\n",
    "\n",
    "    for GP_class in GP_classes:\n",
    "        print(f\"Training {GP_class.__name__}...\")\n",
    "        \n",
    "        # Run the experiment N times and store results for each run\n",
    "        all_runs_rmse = []\n",
    "        all_runs_runtimes = []\n",
    "        \n",
    "        for run in range(N):\n",
    "            epoch_rmse, epoch_runtimes = train_gp(\n",
    "                GP_class,\n",
    "                inducing_points.clone(),\n",
    "                test_dataset,\n",
    "                train_features,\n",
    "                train_labels,\n",
    "                epochs,\n",
    "                device,\n",
    "                dtype\n",
    "            )\n",
    "            all_runs_rmse.append(epoch_rmse)\n",
    "            all_runs_runtimes.append(epoch_runtimes)\n",
    "\n",
    "        # Calculate mean and std deviation across the N runs\n",
    "        mean_rmse = np.mean(all_runs_rmse, axis=0)\n",
    "        std_rmse = np.std(all_runs_rmse, axis=0)\n",
    "        mean_runtimes = np.mean(all_runs_runtimes, axis=0)\n",
    "        std_runtimes = np.std(all_runs_runtimes, axis=0)\n",
    "\n",
    "        all_mean_rmse.append(mean_rmse)\n",
    "        all_mean_runtimes.append(mean_runtimes)\n",
    "        all_std_rmse.append(std_rmse)\n",
    "        all_std_runtimes.append(std_runtimes)\n",
    "\n",
    "    return all_mean_rmse, all_mean_runtimes, all_std_rmse, all_std_runtimes\n",
    "\n",
    "#==================Dataset============================\n",
    "from data.get_uci import ElevatorsDataset\n",
    "dataset = ElevatorsDataset(\"../data/uci_datasets/uci_datasets/pol/data.csv\")\n",
    "train_dataset, val_dataset, test_dataset = split_dataset(\n",
    "    dataset,\n",
    "    train_frac=9/10,\n",
    "    val_frac=0/10  \n",
    ")\n",
    "\n",
    "#==================Benchmark============================\n",
    "GP_classes = [SoftGP_baseline, SoftGP_test] \n",
    "epochs = 10\n",
    "N = 1  # Number of runs\n",
    "all_mean_rmse, all_mean_runtimes, all_std_rmse, all_std_runtimes = benchmark(GP_classes, train_dataset, test_dataset, epochs=epochs, seed=42, N=N)\n",
    "plot_results(GP_classes, all_mean_rmse, all_mean_runtimes, all_std_rmse, all_std_runtimes, epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character '█' (U+2588) (2939239835.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[7], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    100%|██████████| 2/2 [00:01<00:00,  1.95it/s]\u001b[0m\n\u001b[1;37m         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid character '█' (U+2588)\n"
     ]
    }
   ],
   "source": [
    "SIZE (15000, 27)\n",
    "100%|██████████| 2/2 [00:01<00:00,  1.95it/s]\n",
    "Training SoftGP_baseline...\n",
    "USING QR BATCH\n",
    "100%|██████████| 47/47 [00:06<00:00,  7.74it/s]\n",
    "RMSE: 0.5389598608016968 NEG_MLL 45598.875 NOISE 0.0010000000474974513 LENGTHSCALE tensor([[0.6435]], grad_fn=<SoftplusBackward0>) OUTPUTSCALE 1.0\n",
    "USING QR BATCH\n",
    "100%|██████████| 47/47 [00:06<00:00,  7.42it/s]\n",
    "RMSE: 0.5087882876396179 NEG_MLL 41339.1015625 NOISE 0.0010000000474974513 LENGTHSCALE tensor([[0.6068]], grad_fn=<SoftplusBackward0>) OUTPUTSCALE 1.0\n",
    "USING QR BATCH\n",
    "100%|██████████| 47/47 [00:06<00:00,  7.20it/s]\n",
    "RMSE: 0.48444491624832153 NEG_MLL 38776.59765625 NOISE 0.0010000000474974513 LENGTHSCALE tensor([[0.5828]], grad_fn=<SoftplusBackward0>) OUTPUTSCALE 1.0\n",
    "Training SoftGP_test...\n",
    "USING QR BATCH\n",
    "100%|██████████| 47/47 [00:04<00:00,  9.42it/s]\n",
    "RMSE: 0.49580860137939453 NEG_MLL 29874.55859375 NOISE 0.0010000000474974513 LENGTHSCALE tensor([[0.7512]], grad_fn=<SoftplusBackward0>) OUTPUTSCALE 1.0\n",
    "USING QR BATCH\n",
    "100%|██████████| 47/47 [00:04<00:00, 10.42it/s]\n",
    "RMSE: 0.48252424597740173 NEG_MLL 30173.1953125 NOISE 0.0010000000474974513 LENGTHSCALE tensor([[0.8222]], grad_fn=<SoftplusBackward0>) OUTPUTSCALE 1.0\n",
    "USING QR BATCH\n",
    "100%|██████████| 47/47 [00:04<00:00, 10.80it/s]\n",
    "RMSE: 0.4675379991531372 NEG_MLL 29973.189453125 NOISE 0.0010000000474974513 LENGTHSCALE tensor([[0.9045]], grad_fn=<SoftplusBackward0>) OUTPUTSCALE 1.0\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "softgp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
