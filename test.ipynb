{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f6990743330>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from itertools import product\n",
    "from functools import partial\n",
    "from torch.autograd.functional import jacobian\n",
    "\n",
    "torch.manual_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0819, -0.0220, -0.0599],\n",
      "        [-0.0220,  0.1848, -0.1628],\n",
      "        [-0.0599, -0.1628,  0.2227]])\n",
      "tensor([[ 0.2194, -0.0767, -0.1427],\n",
      "        [-0.0767,  0.1803, -0.1036],\n",
      "        [-0.1427, -0.1036,  0.2463]])\n"
     ]
    }
   ],
   "source": [
    "# x = torch.arange(3, dtype=torch.float32, requires_grad=True)\n",
    "x = torch.tensor([\n",
    "    [1, 2, 3],\n",
    "    [.6, .28, .9]\n",
    "], requires_grad=True)\n",
    "\n",
    "\n",
    "def f(x):\n",
    "    return torch.softmax(x, dim=-1)\n",
    "\n",
    "auto = jacobian(f, x[0])\n",
    "print(auto)\n",
    "auto = jacobian(f, x[1])\n",
    "print(auto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3]) tensor([[0.0900, 0.2447, 0.6652],\n",
      "        [0.3251, 0.2361, 0.4388]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3, 3]),\n",
       " tensor([[[ 0.0819, -0.0220, -0.0599],\n",
       "          [-0.0220,  0.1848, -0.1628],\n",
       "          [-0.0599, -0.1628,  0.2227]],\n",
       " \n",
       "         [[ 0.2194, -0.0767, -0.1427],\n",
       "          [-0.0767,  0.1803, -0.1036],\n",
       "          [-0.1427, -0.1036,  0.2463]]]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N, M = x.shape\n",
    "\n",
    "P = torch.softmax(x, dim=-1)\n",
    "print(P.shape, P)\n",
    "manual = torch.tensor([[[P[i, j] * ((1 if j == k else 0) - P[i, k]) for k in range(M)] for j in range(M)] for i in range(N)])\n",
    "manual.shape, manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3]) tensor([[0.0900, 0.2447, 0.6652],\n",
      "        [0.3251, 0.2361, 0.4388]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3, 3]),\n",
       " tensor([[[ 0.0819, -0.0220, -0.0599],\n",
       "          [-0.0220,  0.1848, -0.1628],\n",
       "          [-0.0599, -0.1628,  0.2227]],\n",
       " \n",
       "         [[ 0.2194, -0.0767, -0.1427],\n",
       "          [-0.0767,  0.1803, -0.1036],\n",
       "          [-0.1427, -0.1036,  0.2463]]], grad_fn=<MulBackward0>))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def softmax_deriv(P):\n",
    "    # P is of shape (N, M)\n",
    "    N, M = P.shape\n",
    "\n",
    "    # Create a Kronecker delta (identity matrix) of shape (M, M)\n",
    "    delta = torch.eye(M, dtype=P.dtype, device=P.device)\n",
    "\n",
    "    # Expand P to have an extra dimension for broadcasting\n",
    "    P_ik = P.unsqueeze(2)  # Shape: (N, M, 1)\n",
    "    P_ij = P.unsqueeze(1)  # Shape: (N, 1, M)\n",
    "\n",
    "    # Compute the expression: p_{ij} * (delta_{j=k} - p_{ik})\n",
    "    # Here, delta expands along axis 0 (broadcasted across the batch dimension)\n",
    "    return P_ij * (delta - P_ik)\n",
    "\n",
    "print(P.shape, P)\n",
    "softmax_deriv(P).shape, softmax_deriv(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6332,  0.2451,  0.0506,  0.0711],\n",
       "        [ 0.0471,  0.0192,  0.5049,  0.4288],\n",
       "        [-0.1341, -0.0989,  0.0249,  0.0308],\n",
       "        [-0.1341, -0.0494,  0.0125,  0.0168],\n",
       "        [-0.1341, -0.1483,  0.0391,  0.0560],\n",
       "        [-0.0067, -0.0038,  0.0411,  0.0193],\n",
       "        [-0.0060, -0.0027,  0.0000, -0.0064],\n",
       "        [-0.0440, -0.0183, -0.2466, -0.2441]], grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = torch.tensor([\n",
    "    [1, 2, 3],\n",
    "    [1.5, 2.2, 3.8],\n",
    "    [-.5, 1.2, 0.7],\n",
    "    [-.2, 1.3, 0.9],\n",
    "], requires_grad=True)\n",
    "x = torch.tensor([\n",
    "    [0.9, 1.9, 2.9], \n",
    "    [0.1, 1.2, -2.9], \n",
    "], requires_grad=True)\n",
    "\n",
    "# B x M x D\n",
    "x_expanded = x.unsqueeze(1).expand(-1, z.shape[0], -1)        \n",
    "diff = x_expanded - z\n",
    "distances = torch.linalg.vector_norm(diff, ord=2, dim=-1)\n",
    "softmax_distances = torch.softmax(-distances, dim=-1)\n",
    "W_xz = softmax_distances\n",
    "softmax_deriv = softmax_distances * (1 - softmax_distances)\n",
    "W_xz_deriv = (softmax_deriv.unsqueeze(-1) * diff / distances.unsqueeze(-1)).transpose(2, 1)\n",
    "torch.cat([W_xz, W_xz_deriv.reshape(-1, z.shape[0])], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6332,  0.2451,  0.0506,  0.0711],\n",
       "        [ 0.0471,  0.0192,  0.5049,  0.4288],\n",
       "        [-0.1341, -0.0989,  0.0249,  0.0308],\n",
       "        [-0.1341, -0.0494,  0.0125,  0.0168],\n",
       "        [-0.1341, -0.1483,  0.0391,  0.0560],\n",
       "        [-0.0067, -0.0038,  0.0411,  0.0193],\n",
       "        [-0.0060, -0.0027,  0.0000, -0.0064],\n",
       "        [-0.0440, -0.0183, -0.2466, -0.2441]], grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(x, z):\n",
    "    distances = torch.zeros(4)\n",
    "    distances[0] = torch.linalg.vector_norm(x - z[0], ord=2, dim=0)\n",
    "    distances[1] = torch.linalg.vector_norm(x - z[1], ord=2, dim=0)\n",
    "    distances[2] = torch.linalg.vector_norm(x - z[2], ord=2, dim=0)\n",
    "    distances[3] = torch.linalg.vector_norm(x - z[3], ord=2, dim=0)\n",
    "    # distances = torch.tensor([torch.linalg.vector_norm(x - z[i], ord=2, dim=0) for i in range(4)])\n",
    "    return torch.softmax(-distances, dim=-1)\n",
    "\n",
    "# print(f(x[0], z))\n",
    "# print(f(x[1], z))\n",
    "# print(jacobian(partial(f, x[0]), z).diagonal(dim1=0, dim2=1))\n",
    "# print(jacobian(partial(f, x[1]), z).diagonal(dim1=0, dim2=1))\n",
    "\n",
    "torch.cat([\n",
    "    f(x[0], z).unsqueeze(0),\n",
    "    f(x[1], z).unsqueeze(0),\n",
    "    jacobian(partial(f, x[0]), z).diagonal(dim1=0, dim2=1),\n",
    "    jacobian(partial(f, x[1]), z).diagonal(dim1=0, dim2=1),\n",
    "], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "softgp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
